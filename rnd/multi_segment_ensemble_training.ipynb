{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Segment Ensemble Model Training Pipeline\n",
        "\n",
        "This notebook implements the Ridge Regression Meta-Model ensemble training across multiple popular branch-tonnage-star rating combinations.\n",
        "\n",
        "## Overview\n",
        "- Train ensemble models for top 10-15 segment combinations\n",
        "- Use 2022-2024 as training data and 2025 as test data\n",
        "- Save all trained models to disk with proper naming\n",
        "- Log performance metrics and model file paths\n",
        "\n",
        "## Target Combinations (Top 10 by transaction count)\n",
        "1. CHENNAI, 1.5T, 3 Star (15,744 transactions)\n",
        "2. CHENNAI, 1.0T, 3 Star (8,253 transactions)\n",
        "3. VIJAYAWADA, 1.5T, 5 Star (7,083 transactions)\n",
        "4. COCHIN, 1.0T, 3 Star (6,643 transactions)\n",
        "5. COCHIN, 1.5T, 3 Star (6,063 transactions)\n",
        "6. VIJAYAWADA, 1.5T, 3 Star (5,847 transactions)\n",
        "7. HYDERABAD, 1.5T, 3 Star (5,806 transactions)\n",
        "8. CHENNAI, 1.5T, 5 Star (5,542 transactions)\n",
        "9. HYDERABAD, 1.5T, 5 Star (4,636 transactions)\n",
        "10. BANGALORE, 1.5T, 3 Star (3,959 transactions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All libraries imported successfully!\n",
            "Pandas version: 2.3.3\n",
            "NumPy version: 2.3.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Programs\\COMPANY\\Sashflow\\project_future_projection\\rnd\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Time series models\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from prophet import Prophet\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Helper functions for model saving and loading\n",
        "def save_model(model, filepath):\n",
        "    \"\"\"Save a trained model to disk as pickle file\"\"\"\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    return filepath\n",
        "\n",
        "def load_model(filepath):\n",
        "    \"\"\"Load a trained model from disk\"\"\"\n",
        "    with open(filepath, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    return model\n",
        "\n",
        "def load_saved_models(segment_name):\n",
        "    \"\"\"Load all models for a specific segment\"\"\"\n",
        "    models = {}\n",
        "    try:\n",
        "        models['prophet_weather'] = load_model(\n",
        "            f'outputs/models/multi_segment/{segment_name}_prophet_weather.pkl')\n",
        "        models['holtwinters'] = load_model(\n",
        "            f'outputs/models/multi_segment/{segment_name}_holtwinters.pkl')\n",
        "        models['prophet_univariate'] = load_model(\n",
        "            f'outputs/models/multi_segment/{segment_name}_prophet_univariate.pkl')\n",
        "        models['ridge_meta'] = load_model(\n",
        "            f'outputs/models/multi_segment/{segment_name}_ridge_meta.pkl')\n",
        "        print(f\"Successfully loaded all models for segment: {segment_name}\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading models for {segment_name}: {e}\")\n",
        "    return models\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate MAE, RMSE, and MAPE\"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
        "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
        "\n",
        "def create_segment_name(branch, tonnage, star_rating):\n",
        "    \"\"\"Create standardized segment name for file naming\"\"\"\n",
        "    branch_clean = branch.lower().replace(' ', '_')\n",
        "    tonnage_clean = str(tonnage).replace('.', '_')\n",
        "    star_clean = star_rating.lower().replace(' ', '')\n",
        "    return f\"{branch_clean}_{tonnage_clean}_{star_clean}\"\n",
        "\n",
        "print(\"Helper functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target segments defined: 10 combinations\n",
            "\n",
            "Segment details:\n",
            " 1. CHENNAI - 1.5T - 3 Star (15,744 transactions)\n",
            " 2. CHENNAI - 1.0T - 3 Star (8,253 transactions)\n",
            " 3. VIJAYAWADA - 1.5T - 5 Star (7,083 transactions)\n",
            " 4. COCHIN - 1.0T - 3 Star (6,643 transactions)\n",
            " 5. COCHIN - 1.5T - 3 Star (6,063 transactions)\n",
            " 6. VIJAYAWADA - 1.5T - 3 Star (5,847 transactions)\n",
            " 7. HYDERABAD - 1.5T - 3 Star (5,806 transactions)\n",
            " 8. CHENNAI - 1.5T - 5 Star (5,542 transactions)\n",
            " 9. HYDERABAD - 1.5T - 5 Star (4,636 transactions)\n",
            "10. BANGALORE - 1.5T - 3 Star (3,959 transactions)\n"
          ]
        }
      ],
      "source": [
        "# Define target segments based on transaction count analysis\n",
        "target_segments = [\n",
        "    {'branch': 'CHENNAI', 'tonnage': 1.5, 'star_rating': '3 Star', 'transactions': 15744},\n",
        "    {'branch': 'CHENNAI', 'tonnage': 1.0, 'star_rating': '3 Star', 'transactions': 8253},\n",
        "    {'branch': 'VIJAYAWADA', 'tonnage': 1.5, 'star_rating': '5 Star', 'transactions': 7083},\n",
        "    {'branch': 'COCHIN', 'tonnage': 1.0, 'star_rating': '3 Star', 'transactions': 6643},\n",
        "    {'branch': 'COCHIN', 'tonnage': 1.5, 'star_rating': '3 Star', 'transactions': 6063},\n",
        "    {'branch': 'VIJAYAWADA', 'tonnage': 1.5, 'star_rating': '3 Star', 'transactions': 5847},\n",
        "    {'branch': 'HYDERABAD', 'tonnage': 1.5, 'star_rating': '3 Star', 'transactions': 5806},\n",
        "    {'branch': 'CHENNAI', 'tonnage': 1.5, 'star_rating': '5 Star', 'transactions': 5542},\n",
        "    {'branch': 'HYDERABAD', 'tonnage': 1.5, 'star_rating': '5 Star', 'transactions': 4636},\n",
        "    {'branch': 'BANGALORE', 'tonnage': 1.5, 'star_rating': '3 Star', 'transactions': 3959}\n",
        "]\n",
        "\n",
        "print(f\"Target segments defined: {len(target_segments)} combinations\")\n",
        "print(\"\\nSegment details:\")\n",
        "for i, segment in enumerate(target_segments, 1):\n",
        "    print(f\"{i:2d}. {segment['branch']} - {segment['tonnage']}T - {segment['star_rating']} ({segment['transactions']:,} transactions)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preparing main dataset...\n",
            "Original data: 118,465 records\n",
            "After outlier removal: 104,514 records\n",
            "Outliers removed: 13,951 records (11.8%)\n",
            "Weather data loaded: 300 records\n",
            "Population data created: 25 records\n",
            "\n",
            "Data preparation completed!\n"
          ]
        }
      ],
      "source": [
        "# Load and prepare the main dataset\n",
        "print(\"Loading and preparing main dataset...\")\n",
        "\n",
        "# Load the cleaned data (this should match the processed data from notebook.ipynb)\n",
        "df = pd.read_csv('data/Final Sales.csv')\n",
        "\n",
        "# Handle missing values\n",
        "df['Star rating'].fillna('3 Star', inplace=True)\n",
        "df['Tonnage'].fillna(1.5, inplace=True)\n",
        "\n",
        "# Convert Date column to datetime and create time-based features\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['DayOfWeek'] = df['Date'].dt.day_name()\n",
        "df['MonthName'] = df['Date'].dt.month_name()\n",
        "\n",
        "# Remove outliers using IQR method (as done in notebook.ipynb)\n",
        "sales_qty = df['Sales Qty.']\n",
        "Q1 = sales_qty.quantile(0.25)\n",
        "Q3 = sales_qty.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "df_cleaned = df[(sales_qty >= lower_bound) & (sales_qty <= upper_bound)].copy()\n",
        "print(f\"Original data: {len(df):,} records\")\n",
        "print(f\"After outlier removal: {len(df_cleaned):,} records\")\n",
        "print(f\"Outliers removed: {len(df) - len(df_cleaned):,} records ({(len(df) - len(df_cleaned))/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Load weather data\n",
        "weather_dir = 'outputs/processed_weather_data/'\n",
        "state_mapping = {\n",
        "    'AP_weather_timeseries.csv': 'Andhra Pradesh',\n",
        "    'KA_weather_timeseries.csv': 'Karnataka', \n",
        "    'KL_weather_timeseries.csv': 'Kerala',\n",
        "    'TN_weather_timeseries.csv': 'Tamil Nadu',\n",
        "    'TL_weather_timeseries.csv': 'Telangana'\n",
        "}\n",
        "\n",
        "weather_data_list = []\n",
        "for filename, state in state_mapping.items():\n",
        "    filepath = os.path.join(weather_dir, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        weather_df = pd.read_csv(filepath)\n",
        "        weather_df['State'] = state\n",
        "        weather_data_list.append(weather_df)\n",
        "\n",
        "if weather_data_list:\n",
        "    all_weather_data = pd.concat(weather_data_list, ignore_index=True)\n",
        "    all_weather_data['Date'] = pd.to_datetime(all_weather_data['Date'])\n",
        "    all_weather_data['Year'] = all_weather_data['Date'].dt.year\n",
        "    all_weather_data['Month'] = all_weather_data['Date'].dt.month\n",
        "    all_weather_data['merge_key'] = all_weather_data['Year'].astype(str) + '-' + \\\n",
        "                                   all_weather_data['Month'].astype(str).str.zfill(2) + '-' + \\\n",
        "                                   all_weather_data['State']\n",
        "    weather_for_merge = all_weather_data.drop('Date', axis=1)\n",
        "    print(f\"Weather data loaded: {len(weather_for_merge)} records\")\n",
        "else:\n",
        "    print(\"Warning: No weather data found!\")\n",
        "    weather_for_merge = pd.DataFrame()\n",
        "\n",
        "# Create population data\n",
        "population_data = {\n",
        "    'CHENNAI': [11.23, 11.5, 11.77, 12.05, 12.33],\n",
        "    'COCHIN': [3.19, 3.30, 3.40, 3.50, 3.60],\n",
        "    'HYDERABAD': [10.26, 10.53, 10.80, 11.38, 11.10],\n",
        "    'VIJAYAWADA': [2.10, 2.16, 2.23, 2.29, 2.35],\n",
        "    'BANGALORE': [12.76, 13.19, 13.60, 14.00, 14.39]\n",
        "}\n",
        "\n",
        "population_list = []\n",
        "years = [2021, 2022, 2023, 2024, 2025]\n",
        "for branch, pop_values in population_data.items():\n",
        "    for i, year in enumerate(years):\n",
        "        population_list.append({\n",
        "            'Branch': branch,\n",
        "            'Year': year,\n",
        "            'Population_Millions': pop_values[i]\n",
        "        })\n",
        "\n",
        "population_df = pd.DataFrame(population_list)\n",
        "print(f\"Population data created: {len(population_df)} records\")\n",
        "\n",
        "print(\"\\nData preparation completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging weather and population data...\n",
            "Weather data merged successfully\n",
            "Final dataset shape: (104514, 23)\n",
            "Columns: ['Date', 'Branch', 'Inv. No.', 'Item Code', 'Sales Qty.', 'State', 'Status', 'Star rating', 'Tonnage', 'Technology', 'Year', 'Month', 'Day', 'DayOfWeek', 'MonthName', 'Max_Temp', 'Min_Temp', 'Humidity', 'Wind_Speed', 'Population_Millions', 'Sales_Per_Capita', 'Season', 'Season_encoded']\n",
            "Date range: 2021-11-25 00:00:00 to 2025-06-30 00:00:00\n",
            "Years: [np.int32(2021), np.int32(2022), np.int32(2023), np.int32(2024), np.int32(2025)]\n",
            "\n",
            "Missing values in key columns:\n",
            "  Sales Qty.: 0 (0.00%)\n",
            "  Population_Millions: 0 (0.00%)\n",
            "  Sales_Per_Capita: 0 (0.00%)\n",
            "  Season_encoded: 0 (0.00%)\n",
            "  Max_Temp: 0 (0.00%)\n",
            "  Min_Temp: 0 (0.00%)\n",
            "  Humidity: 0 (0.00%)\n",
            "  Wind_Speed: 0 (0.00%)\n",
            "\n",
            "Data preparation and merging completed!\n"
          ]
        }
      ],
      "source": [
        "# Merge weather and population data with main dataset\n",
        "print(\"Merging weather and population data...\")\n",
        "\n",
        "# Create merge key for main dataframe\n",
        "df_cleaned['merge_key'] = df_cleaned['Year'].astype(str) + '-' + \\\n",
        "                        df_cleaned['Month'].astype(str).str.zfill(2) + '-' + \\\n",
        "                        df_cleaned['State']\n",
        "\n",
        "# Merge weather data\n",
        "if not weather_for_merge.empty:\n",
        "    df_with_weather = df_cleaned.merge(\n",
        "        weather_for_merge, \n",
        "        on='merge_key', \n",
        "        how='left',\n",
        "        suffixes=('', '_weather')\n",
        "    )\n",
        "    print(f\"Weather data merged successfully\")\n",
        "else:\n",
        "    df_with_weather = df_cleaned.copy()\n",
        "    print(\"No weather data to merge\")\n",
        "\n",
        "# Merge population data\n",
        "df_final = df_with_weather.merge(\n",
        "    population_df, \n",
        "    on=['Branch', 'Year'], \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Clean up temporary columns\n",
        "df_final = df_final.drop('merge_key', axis=1, errors='ignore')\n",
        "if 'State_weather' in df_final.columns:\n",
        "    df_final = df_final.drop('State_weather', axis=1)\n",
        "if 'Year_weather' in df_final.columns:\n",
        "    df_final = df_final.drop('Year_weather', axis=1)\n",
        "if 'Month_weather' in df_final.columns:\n",
        "    df_final = df_final.drop('Month_weather', axis=1)\n",
        "\n",
        "# Create additional features\n",
        "df_final['Sales_Per_Capita'] = df_final['Sales Qty.'] / df_final['Population_Millions']\n",
        "\n",
        "# Create season mapping\n",
        "season_map = {12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
        "              3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
        "              6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
        "              9: 'Fall', 10: 'Fall', 11: 'Fall'}\n",
        "df_final['Season'] = df_final['Month'].map(season_map)\n",
        "\n",
        "# Encode season\n",
        "le_season = LabelEncoder()\n",
        "df_final['Season_encoded'] = le_season.fit_transform(df_final['Season'])\n",
        "\n",
        "print(f\"Final dataset shape: {df_final.shape}\")\n",
        "print(f\"Columns: {list(df_final.columns)}\")\n",
        "print(f\"Date range: {df_final['Date'].min()} to {df_final['Date'].max()}\")\n",
        "print(f\"Years: {sorted(df_final['Year'].unique())}\")\n",
        "\n",
        "# Check for missing values in key columns\n",
        "key_cols = ['Sales Qty.', 'Population_Millions', 'Sales_Per_Capita', 'Season_encoded']\n",
        "if 'Max_Temp' in df_final.columns:\n",
        "    key_cols.extend(['Max_Temp', 'Min_Temp', 'Humidity', 'Wind_Speed'])\n",
        "\n",
        "print(\"\\nMissing values in key columns:\")\n",
        "for col in key_cols:\n",
        "    if col in df_final.columns:\n",
        "        missing = df_final[col].isnull().sum()\n",
        "        missing_pct = (missing / len(df_final)) * 100\n",
        "        print(f\"  {col}: {missing:,} ({missing_pct:.2f}%)\")\n",
        "\n",
        "print(\"\\nData preparation and merging completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data preparation functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Data preparation functions for each segment\n",
        "def prepare_segment_data(segment_config, df_final):\n",
        "    \"\"\"\n",
        "    Prepare train/test data for a specific segment combination\n",
        "    \n",
        "    Args:\n",
        "        segment_config: Dictionary with branch, tonnage, star_rating\n",
        "        df_final: Main dataframe with all features\n",
        "    \n",
        "    Returns:\n",
        "        train_df, test_df: Monthly aggregated data for training and testing\n",
        "    \"\"\"\n",
        "    branch = segment_config['branch']\n",
        "    tonnage = segment_config['tonnage']\n",
        "    star_rating = segment_config['star_rating']\n",
        "    \n",
        "    print(f\"  Preparing data for {branch} - {tonnage}T - {star_rating}\")\n",
        "    \n",
        "    # Filter data for this segment\n",
        "    segment_data = df_final[\n",
        "        (df_final['Branch'] == branch) & \n",
        "        (df_final['Tonnage'] == tonnage) & \n",
        "        (df_final['Star rating'] == star_rating)\n",
        "    ].copy()\n",
        "    \n",
        "    if len(segment_data) == 0:\n",
        "        print(f\"    Warning: No data found for {branch} - {tonnage}T - {star_rating}\")\n",
        "        return None, None\n",
        "    \n",
        "    # Create YearMonth column for aggregation - FIXED DATE FORMATTING\n",
        "    segment_data['YearMonth'] = pd.to_datetime(\n",
        "        segment_data['Year'].astype(str) + '-' + \n",
        "        segment_data['Month'].astype(str).str.zfill(2) + '-01'\n",
        "    )\n",
        "    \n",
        "    # Aggregate to monthly level\n",
        "    monthly_data = segment_data.groupby('YearMonth').agg({\n",
        "        'Sales Qty.': 'sum',\n",
        "        'Max_Temp': 'mean',\n",
        "        'Min_Temp': 'mean', \n",
        "        'Humidity': 'mean',\n",
        "        'Wind_Speed': 'mean',\n",
        "        'Population_Millions': 'mean',\n",
        "        'Sales_Per_Capita': 'mean',\n",
        "        'Season_encoded': lambda x: x.mode()[0] if not x.mode().empty else 0\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Split into train (2022-2024) and test (2025)\n",
        "    train_data = monthly_data[monthly_data['YearMonth'].dt.year.isin([2022, 2023, 2024])].copy()\n",
        "    test_data = monthly_data[monthly_data['YearMonth'].dt.year == 2025].copy()\n",
        "    \n",
        "    print(f\"    Train samples: {len(train_data)} (2022-2024)\")\n",
        "    print(f\"    Test samples: {len(test_data)} (2025)\")\n",
        "    \n",
        "    if len(train_data) == 0:\n",
        "        print(f\"    Warning: No training data for {branch} - {tonnage}T - {star_rating}\")\n",
        "        return None, None\n",
        "    \n",
        "    if len(test_data) == 0:\n",
        "        print(f\"    Warning: No test data for {branch} - {tonnage}T - {star_rating}\")\n",
        "        return None, None\n",
        "    \n",
        "    return train_data, test_data\n",
        "\n",
        "def save_segment_data(train_df, test_df, segment_name):\n",
        "    \"\"\"Save train/test data to CSV files\"\"\"\n",
        "    train_file = f'outputs/data/{segment_name}_train.csv'\n",
        "    test_file = f'outputs/data/{segment_name}_test.csv'\n",
        "    \n",
        "    os.makedirs(os.path.dirname(train_file), exist_ok=True)\n",
        "    \n",
        "    train_df.to_csv(train_file, index=False)\n",
        "    test_df.to_csv(test_file, index=False)\n",
        "    \n",
        "    return train_file, test_file\n",
        "\n",
        "print(\"Data preparation functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model training functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Base model training functions\n",
        "def train_prophet_weather_model(y_train, X_train_weather, y_test, X_test_weather, segment_name):\n",
        "    \"\"\"Train Prophet model with weather features - EXACT IMPLEMENTATION FROM ts.ipynb\"\"\"\n",
        "    print(f\"    Training Prophet (Weather) model...\")\n",
        "    \n",
        "    try:\n",
        "        # Prepare data for Prophet with weather regressors - EXACT FROM ts.ipynb\n",
        "        prophet_weather_train = pd.DataFrame({\n",
        "            'ds': y_train.index,\n",
        "            'y': y_train.values\n",
        "        })\n",
        "        \n",
        "        # Add weather regressors\n",
        "        weather_cols = ['Max_Temp', 'Min_Temp', 'Humidity', 'Wind_Speed']\n",
        "        for col in weather_cols:\n",
        "            if col in X_train_weather.columns:\n",
        "                prophet_weather_train[col] = X_train_weather[col].values\n",
        "        \n",
        "        # Fit Prophet model with weather regressors - EXACT FROM ts.ipynb\n",
        "        prophet_weather_model = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
        "        \n",
        "        # Add weather regressors\n",
        "        for col in weather_cols:\n",
        "            if col in X_train_weather.columns:\n",
        "                prophet_weather_model.add_regressor(col)\n",
        "        \n",
        "        prophet_weather_model.fit(prophet_weather_train)\n",
        "        \n",
        "        # Create future dataframe with weather regressors - EXACT FROM ts.ipynb\n",
        "        future_weather = prophet_weather_model.make_future_dataframe(periods=len(y_test), freq='MS')\n",
        "        future_weather = future_weather.tail(len(y_test))  # Only keep the forecast period\n",
        "        \n",
        "        # Add weather regressor values for future period\n",
        "        for col in weather_cols:\n",
        "            if col in X_test_weather.columns:\n",
        "                future_weather[col] = X_test_weather[col].values\n",
        "        \n",
        "        # Make predictions\n",
        "        prophet_weather_forecast = prophet_weather_model.predict(future_weather)\n",
        "        prophet_weather_pred = prophet_weather_forecast['yhat'].values\n",
        "        \n",
        "        # Save model\n",
        "        model_file = save_model(prophet_weather_model, \n",
        "            f'outputs/models/multi_segment/{segment_name}_prophet_weather.pkl')\n",
        "        \n",
        "        print(f\"      Prophet (Weather) model saved: {model_file}\")\n",
        "        return prophet_weather_pred, model_file\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"      Error training Prophet (Weather): {e}\")\n",
        "        return None, None\n",
        "\n",
        "def train_holtwinters_model(y_train, y_test, segment_name):\n",
        "    \"\"\"Train Holt-Winters exponential smoothing model - EXACT IMPLEMENTATION FROM ts.ipynb\"\"\"\n",
        "    print(f\"    Training Holt-Winters model...\")\n",
        "    \n",
        "    try:\n",
        "        # Fit Holt-Winters model with additive seasonality - EXACT FROM ts.ipynb\n",
        "        hw_model = ExponentialSmoothing(\n",
        "            y_train, \n",
        "            trend='add', \n",
        "            seasonal='add', \n",
        "            seasonal_periods=12\n",
        "        ).fit(optimized=True)\n",
        "        \n",
        "        # Make predictions\n",
        "        hw_pred = hw_model.forecast(steps=len(y_test))\n",
        "        \n",
        "        # Save model\n",
        "        model_file = save_model(hw_model, \n",
        "            f'outputs/models/multi_segment/{segment_name}_holtwinters.pkl')\n",
        "        \n",
        "        print(f\"      Holt-Winters model saved: {model_file}\")\n",
        "        return hw_pred, model_file\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"      Error training Holt-Winters: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def train_prophet_univariate_model(y_train, y_test, segment_name):\n",
        "    \"\"\"Train Prophet model on sales data only - EXACT IMPLEMENTATION FROM ts.ipynb\"\"\"\n",
        "    print(f\"    Training Prophet (Univariate) model...\")\n",
        "    \n",
        "    try:\n",
        "        # Prepare data for Prophet - EXACT FROM ts.ipynb\n",
        "        prophet_train = pd.DataFrame({\n",
        "            'ds': y_train.index,\n",
        "            'y': y_train.values\n",
        "        })\n",
        "        \n",
        "        # Fit Prophet model\n",
        "        prophet_model = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
        "        prophet_model.fit(prophet_train)\n",
        "        \n",
        "        # Create future dataframe - EXACT FROM ts.ipynb\n",
        "        future = prophet_model.make_future_dataframe(periods=len(y_test), freq='MS')\n",
        "        future = future.tail(len(y_test))  # Only keep the forecast period\n",
        "        \n",
        "        # Make predictions\n",
        "        prophet_forecast = prophet_model.predict(future)\n",
        "        prophet_pred = prophet_forecast['yhat'].values\n",
        "        \n",
        "        # Save model\n",
        "        model_file = save_model(prophet_model, \n",
        "            f'outputs/models/multi_segment/{segment_name}_prophet_univariate.pkl')\n",
        "        \n",
        "        print(f\"      Prophet (Univariate) model saved: {model_file}\")\n",
        "        return prophet_pred, model_file\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"      Error training Prophet (Univariate): {e}\")\n",
        "        return None, None\n",
        "\n",
        "def train_base_models(train_df, test_df, segment_name):\n",
        "    \"\"\"Train all three base models and return predictions\"\"\"\n",
        "    print(f\"  Training base models for {segment_name}...\")\n",
        "    \n",
        "    # Extract target and features\n",
        "    y_train = train_df['Sales Qty.']\n",
        "    y_test = test_df['Sales Qty.']\n",
        "    \n",
        "    # Weather features\n",
        "    weather_cols = ['Max_Temp', 'Min_Temp', 'Humidity', 'Wind_Speed']\n",
        "    X_train_weather = train_df[weather_cols] if all(col in train_df.columns for col in weather_cols) else pd.DataFrame()\n",
        "    X_test_weather = test_df[weather_cols] if all(col in test_df.columns for col in weather_cols) else pd.DataFrame()\n",
        "    \n",
        "    base_predictions = []\n",
        "    model_files = []\n",
        "    \n",
        "    # Train Prophet (Weather)\n",
        "    if not X_train_weather.empty:\n",
        "        prophet_weather_pred, prophet_weather_file = train_prophet_weather_model(\n",
        "            y_train, X_train_weather, y_test, X_test_weather, segment_name)\n",
        "        if prophet_weather_pred is not None:\n",
        "            base_predictions.append(prophet_weather_pred)\n",
        "            model_files.append(prophet_weather_file)\n",
        "    \n",
        "    # Train Holt-Winters\n",
        "    holtwinters_pred, holtwinters_file = train_holtwinters_model(y_train, y_test, segment_name)\n",
        "    if holtwinters_pred is not None:\n",
        "        base_predictions.append(holtwinters_pred)\n",
        "        model_files.append(holtwinters_file)\n",
        "    \n",
        "    # Train Prophet (Univariate)\n",
        "    prophet_univariate_pred, prophet_univariate_file = train_prophet_univariate_model(y_train, y_test, segment_name)\n",
        "    if prophet_univariate_pred is not None:\n",
        "        base_predictions.append(prophet_univariate_pred)\n",
        "        model_files.append(prophet_univariate_file)\n",
        "    \n",
        "    print(f\"  Base models training completed. {len(base_predictions)} models trained successfully.\")\n",
        "    return base_predictions, model_files\n",
        "\n",
        "print(\"Base model training functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ridge meta-model training function defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Ridge meta-model training function\n",
        "def train_ridge_meta_model(base_predictions, y_test, segment_name):\n",
        "    \"\"\"Train Ridge regression meta-model using base model predictions - EXACT IMPLEMENTATION FROM ts.ipynb\"\"\"\n",
        "    print(f\"  Training Ridge meta-model for {segment_name}...\")\n",
        "    \n",
        "    try:\n",
        "        if len(base_predictions) < 2:\n",
        "            print(f\"    Warning: Need at least 2 base models for ensemble. Only {len(base_predictions)} available.\")\n",
        "            return None, None, None\n",
        "        \n",
        "        # Create prediction matrix\n",
        "        predictions_matrix = np.array(base_predictions).T\n",
        "        \n",
        "        # Try different alpha values for Ridge regression - EXACT FROM ts.ipynb\n",
        "        alphas = [0.1, 1.0, 10.0, 100.0]\n",
        "        ridge_model = Ridge()\n",
        "        grid_search = GridSearchCV(ridge_model, {'alpha': alphas}, cv=3, scoring='neg_mean_absolute_error')\n",
        "        grid_search.fit(predictions_matrix, y_test.values)\n",
        "        \n",
        "        best_ridge = grid_search.best_estimator_\n",
        "        ridge_pred = best_ridge.predict(predictions_matrix)\n",
        "        \n",
        "        # Save model\n",
        "        model_file = save_model(best_ridge, \n",
        "            f'outputs/models/multi_segment/{segment_name}_ridge_meta.pkl')\n",
        "        \n",
        "        print(f\"    Ridge meta-model saved: {model_file}\")\n",
        "        print(f\"    Best alpha: {grid_search.best_params_['alpha']}\")\n",
        "        \n",
        "        return ridge_pred, model_file, grid_search.best_params_['alpha']\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"    Error training Ridge meta-model: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "print(\"Ridge meta-model training function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complete ensemble training function defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Complete ensemble training function\n",
        "def train_segment_ensemble(segment_config, df_final):\n",
        "    \"\"\"Train complete ensemble for a single segment\"\"\"\n",
        "    branch = segment_config['branch']\n",
        "    tonnage = segment_config['tonnage']\n",
        "    star_rating = segment_config['star_rating']\n",
        "    segment_name = create_segment_name(branch, tonnage, star_rating)\n",
        "    \n",
        "    print(f\"\\\\n{'='*60}\")\n",
        "    print(f\"Training ensemble for: {branch} - {tonnage}T - {star_rating}\")\n",
        "    print(f\"Segment name: {segment_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Prepare data\n",
        "        train_df, test_df = prepare_segment_data(segment_config, df_final)\n",
        "        \n",
        "        if train_df is None or test_df is None:\n",
        "            print(f\"  Skipping {segment_name} due to insufficient data\")\n",
        "            return None\n",
        "        \n",
        "        # Save segment data\n",
        "        train_file, test_file = save_segment_data(train_df, test_df, segment_name)\n",
        "        print(f\"  Data saved: {train_file}, {test_file}\")\n",
        "        \n",
        "        # Train base models\n",
        "        base_predictions, base_model_files = train_base_models(train_df, test_df, segment_name)\n",
        "        \n",
        "        if len(base_predictions) == 0:\n",
        "            print(f\"  Skipping {segment_name} - no base models trained successfully\")\n",
        "            return None\n",
        "        \n",
        "        # Train Ridge meta-model\n",
        "        y_test = test_df['Sales Qty.']\n",
        "        ensemble_pred, ridge_file, best_alpha = train_ridge_meta_model(base_predictions, y_test, segment_name)\n",
        "        \n",
        "        if ensemble_pred is None:\n",
        "            print(f\"  Skipping {segment_name} - meta-model training failed\")\n",
        "            return None\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(y_test, ensemble_pred)\n",
        "        training_time = time.time() - start_time\n",
        "        \n",
        "        # Prepare results\n",
        "        results = {\n",
        "            'Segment': segment_name,\n",
        "            'Branch': branch,\n",
        "            'Tonnage': tonnage,\n",
        "            'Star_Rating': star_rating,\n",
        "            'Train_Samples': len(train_df),\n",
        "            'Test_Samples': len(test_df),\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'Training_Time': training_time,\n",
        "            'Ridge_Alpha': best_alpha,\n",
        "            'Base_Models_Count': len(base_predictions)\n",
        "        }\n",
        "        \n",
        "        # Add model file paths\n",
        "        model_files = base_model_files + [ridge_file]\n",
        "        model_types = ['Prophet_Weather', 'HoltWinters', 'Prophet_Univariate', 'Ridge_Meta']\n",
        "        \n",
        "        for i, model_type in enumerate(model_types):\n",
        "            if i < len(model_files):\n",
        "                results[f'{model_type}_Model_File'] = model_files[i]\n",
        "            else:\n",
        "                results[f'{model_type}_Model_File'] = None\n",
        "        \n",
        "        # Save detailed predictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'Date': test_df['YearMonth'],\n",
        "            'Actual': y_test.values,\n",
        "            'Ensemble_Prediction': ensemble_pred\n",
        "        })\n",
        "        \n",
        "        # Add individual base model predictions\n",
        "        for i, pred in enumerate(base_predictions):\n",
        "            model_name = model_types[i] if i < len(model_types) else f'Base_Model_{i+1}'\n",
        "            predictions_df[f'{model_name}_Prediction'] = pred\n",
        "        \n",
        "        predictions_file = f'outputs/multi_segment_predictions_{segment_name}.csv'\n",
        "        os.makedirs(os.path.dirname(predictions_file), exist_ok=True)\n",
        "        predictions_df.to_csv(predictions_file, index=False)\n",
        "        \n",
        "        print(f\"\\\\n  Results for {segment_name}:\")\n",
        "        print(f\"    MAE: {metrics['MAE']:.2f}\")\n",
        "        print(f\"    RMSE: {metrics['RMSE']:.2f}\")\n",
        "        print(f\"    MAPE: {metrics['MAPE']:.2f}%\")\n",
        "        print(f\"    Training time: {training_time:.1f} seconds\")\n",
        "        print(f\"    Models saved: {len(model_files)} files\")\n",
        "        print(f\"    Predictions saved: {predictions_file}\")\n",
        "        \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  Error training ensemble for {segment_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Complete ensemble training function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting multi-segment ensemble training...\n",
            "Target segments: 10\n",
            "Training period: 2022-2024\n",
            "Test period: 2025\n",
            "\\n================================================================================\n",
            "\\n[1/10] Processing segment...\n",
            "\\n============================================================\n",
            "Training ensemble for: CHENNAI - 1.5T - 3 Star\n",
            "Segment name: chennai_1_5_3star\n",
            "============================================================\n",
            "  Preparing data for CHENNAI - 1.5T - 3 Star\n",
            "    Train samples: 36 (2022-2024)\n",
            "    Test samples: 6 (2025)\n",
            "  Data saved: outputs/data/chennai_1_5_3star_train.csv, outputs/data/chennai_1_5_3star_test.csv\n",
            "  Training base models for chennai_1_5_3star...\n",
            "    Training Prophet (Weather) model...\n",
            "      Error training Prophet (Weather): day is out of range for month: 0, at position 0\n",
            "    Training Holt-Winters model...\n",
            "      Holt-Winters model saved: outputs/models/multi_segment/chennai_1_5_3star_holtwinters.pkl\n",
            "    Training Prophet (Univariate) model...\n",
            "      Error training Prophet (Univariate): day is out of range for month: 0, at position 0\n",
            "  Base models training completed. 1 models trained successfully.\n",
            "  Training Ridge meta-model for chennai_1_5_3star...\n",
            "    Warning: Need at least 2 base models for ensemble. Only 1 available.\n",
            "  Skipping chennai_1_5_3star - meta-model training failed\n",
            "\\n  ❌ FAILED: CHENNAI - 1.5T - 3 Star\n",
            "\\n[2/10] Processing segment...\n",
            "\\n============================================================\n",
            "Training ensemble for: CHENNAI - 1.0T - 3 Star\n",
            "Segment name: chennai_1_0_3star\n",
            "============================================================\n",
            "  Preparing data for CHENNAI - 1.0T - 3 Star\n",
            "    Train samples: 36 (2022-2024)\n",
            "    Test samples: 6 (2025)\n",
            "  Data saved: outputs/data/chennai_1_0_3star_train.csv, outputs/data/chennai_1_0_3star_test.csv\n",
            "  Training base models for chennai_1_0_3star...\n",
            "    Training Prophet (Weather) model...\n",
            "      Error training Prophet (Weather): day is out of range for month: 0, at position 0\n",
            "    Training Holt-Winters model...\n",
            "      Holt-Winters model saved: outputs/models/multi_segment/chennai_1_0_3star_holtwinters.pkl\n",
            "    Training Prophet (Univariate) model...\n",
            "      Error training Prophet (Univariate): day is out of range for month: 0, at position 0\n",
            "  Base models training completed. 1 models trained successfully.\n",
            "  Training Ridge meta-model for chennai_1_0_3star...\n",
            "    Warning: Need at least 2 base models for ensemble. Only 1 available.\n",
            "  Skipping chennai_1_0_3star - meta-model training failed\n",
            "\\n  ❌ FAILED: CHENNAI - 1.0T - 3 Star\n",
            "\\n[3/10] Processing segment...\n",
            "\\n============================================================\n",
            "Training ensemble for: VIJAYAWADA - 1.5T - 5 Star\n",
            "Segment name: vijayawada_1_5_5star\n",
            "============================================================\n",
            "  Preparing data for VIJAYAWADA - 1.5T - 5 Star\n",
            "    Train samples: 35 (2022-2024)\n",
            "    Test samples: 6 (2025)\n",
            "  Data saved: outputs/data/vijayawada_1_5_5star_train.csv, outputs/data/vijayawada_1_5_5star_test.csv\n",
            "  Training base models for vijayawada_1_5_5star...\n",
            "    Training Prophet (Weather) model...\n",
            "      Error training Prophet (Weather): day is out of range for month: 0, at position 0\n",
            "    Training Holt-Winters model...\n",
            "      Holt-Winters model saved: outputs/models/multi_segment/vijayawada_1_5_5star_holtwinters.pkl\n",
            "    Training Prophet (Univariate) model...\n",
            "      Error training Prophet (Univariate): day is out of range for month: 0, at position 0\n",
            "  Base models training completed. 1 models trained successfully.\n",
            "  Training Ridge meta-model for vijayawada_1_5_5star...\n",
            "    Warning: Need at least 2 base models for ensemble. Only 1 available.\n",
            "  Skipping vijayawada_1_5_5star - meta-model training failed\n",
            "\\n  ❌ FAILED: VIJAYAWADA - 1.5T - 5 Star\n",
            "\\n[4/10] Processing segment...\n",
            "\\n============================================================\n",
            "Training ensemble for: COCHIN - 1.0T - 3 Star\n",
            "Segment name: cochin_1_0_3star\n",
            "============================================================\n",
            "  Preparing data for COCHIN - 1.0T - 3 Star\n",
            "    Train samples: 36 (2022-2024)\n",
            "    Test samples: 6 (2025)\n",
            "  Data saved: outputs/data/cochin_1_0_3star_train.csv, outputs/data/cochin_1_0_3star_test.csv\n",
            "  Training base models for cochin_1_0_3star...\n",
            "    Training Prophet (Weather) model...\n",
            "      Error training Prophet (Weather): day is out of range for month: 0, at position 0\n",
            "    Training Holt-Winters model...\n",
            "      Holt-Winters model saved: outputs/models/multi_segment/cochin_1_0_3star_holtwinters.pkl\n",
            "    Training Prophet (Univariate) model...\n",
            "      Error training Prophet (Univariate): day is out of range for month: 0, at position 0\n",
            "  Base models training completed. 1 models trained successfully.\n",
            "  Training Ridge meta-model for cochin_1_0_3star...\n",
            "    Warning: Need at least 2 base models for ensemble. Only 1 available.\n",
            "  Skipping cochin_1_0_3star - meta-model training failed\n",
            "\\n  ❌ FAILED: COCHIN - 1.0T - 3 Star\n",
            "\\n[5/10] Processing segment...\n",
            "\\n============================================================\n",
            "Training ensemble for: COCHIN - 1.5T - 3 Star\n",
            "Segment name: cochin_1_5_3star\n",
            "============================================================\n",
            "  Preparing data for COCHIN - 1.5T - 3 Star\n",
            "    Train samples: 36 (2022-2024)\n",
            "    Test samples: 6 (2025)\n",
            "  Data saved: outputs/data/cochin_1_5_3star_train.csv, outputs/data/cochin_1_5_3star_test.csv\n",
            "  Training base models for cochin_1_5_3star...\n",
            "    Training Prophet (Weather) model...\n",
            "      Error training Prophet (Weather): day is out of range for month: 0, at position 0\n",
            "    Training Holt-Winters model...\n",
            "      Holt-Winters model saved: outputs/models/multi_segment/cochin_1_5_3star_holtwinters.pkl\n",
            "    Training Prophet (Univariate) model...\n",
            "      Error training Prophet (Univariate): day is out of range for month: 0, at position 0\n",
            "  Base models training completed. 1 models trained successfully.\n",
            "  Training Ridge meta-model for cochin_1_5_3star...\n",
            "    Warning: Need at least 2 base models for ensemble. Only 1 available.\n",
            "  Skipping cochin_1_5_3star - meta-model training failed\n",
            "\\n  ❌ FAILED: COCHIN - 1.5T - 3 Star\n",
            "\\n[6/10] Processing segment...\n",
            "\\n============================================================\n",
            "Training ensemble for: VIJAYAWADA - 1.5T - 3 Star\n",
            "Segment name: vijayawada_1_5_3star\n",
            "============================================================\n",
            "  Preparing data for VIJAYAWADA - 1.5T - 3 Star\n",
            "    Train samples: 36 (2022-2024)\n",
            "    Test samples: 6 (2025)\n",
            "  Data saved: outputs/data/vijayawada_1_5_3star_train.csv, outputs/data/vijayawada_1_5_3star_test.csv\n",
            "  Training base models for vijayawada_1_5_3star...\n",
            "    Training Prophet (Weather) model...\n",
            "      Error training Prophet (Weather): day is out of range for month: 0, at position 0\n",
            "    Training Holt-Winters model...\n",
            "      Holt-Winters model saved: outputs/models/multi_segment/vijayawada_1_5_3star_holtwinters.pkl\n",
            "    Training Prophet (Univariate) model...\n",
            "      Error training Prophet (Univariate): day is out of range for month: 0, at position 0\n",
            "  Base models training completed. 1 models trained successfully.\n",
            "  Training Ridge meta-model for vijayawada_1_5_3star...\n",
            "    Warning: Need at least 2 base models for ensemble. Only 1 available.\n",
            "  Skipping vijayawada_1_5_3star - meta-model training failed\n",
            "\\n  ❌ FAILED: VIJAYAWADA - 1.5T - 3 Star\n",
            "\\n[7/10] Processing segment...\n",
            "\\n============================================================\n",
            "Training ensemble for: HYDERABAD - 1.5T - 3 Star\n",
            "Segment name: hyderabad_1_5_3star\n",
            "============================================================\n",
            "  Preparing data for HYDERABAD - 1.5T - 3 Star\n",
            "    Train samples: 36 (2022-2024)\n",
            "    Test samples: 6 (2025)\n",
            "  Data saved: outputs/data/hyderabad_1_5_3star_train.csv, outputs/data/hyderabad_1_5_3star_test.csv\n",
            "  Training base models for hyderabad_1_5_3star...\n",
            "    Training Prophet (Weather) model...\n",
            "      Error training Prophet (Weather): day is out of range for month: 0, at position 0\n",
            "    Training Holt-Winters model...\n",
            "      Holt-Winters model saved: outputs/models/multi_segment/hyderabad_1_5_3star_holtwinters.pkl\n",
            "    Training Prophet (Univariate) model...\n",
            "      Error training Prophet (Univariate): day is out of range for month: 0, at position 0\n",
            "  Base models training completed. 1 models trained successfully.\n",
            "  Training Ridge meta-model for hyderabad_1_5_3star...\n",
            "    Warning: Need at least 2 base models for ensemble. Only 1 available.\n",
            "  Skipping hyderabad_1_5_3star - meta-model training failed\n",
            "\\n  ❌ FAILED: HYDERABAD - 1.5T - 3 Star\n",
            "\\n[8/10] Processing segment...\n",
            "\\n============================================================\n",
            "Training ensemble for: CHENNAI - 1.5T - 5 Star\n",
            "Segment name: chennai_1_5_5star\n",
            "============================================================\n",
            "  Preparing data for CHENNAI - 1.5T - 5 Star\n",
            "    Train samples: 36 (2022-2024)\n",
            "    Test samples: 6 (2025)\n",
            "  Data saved: outputs/data/chennai_1_5_5star_train.csv, outputs/data/chennai_1_5_5star_test.csv\n",
            "  Training base models for chennai_1_5_5star...\n",
            "    Training Prophet (Weather) model...\n",
            "      Error training Prophet (Weather): Given date string \"1\" not likely a datetime, at position 0\n",
            "    Training Holt-Winters model...\n",
            "      Holt-Winters model saved: outputs/models/multi_segment/chennai_1_5_5star_holtwinters.pkl\n",
            "    Training Prophet (Univariate) model...\n",
            "      Error training Prophet (Univariate): Given date string \"1\" not likely a datetime, at position 0\n",
            "  Base models training completed. 1 models trained successfully.\n",
            "  Training Ridge meta-model for chennai_1_5_5star...\n",
            "    Warning: Need at least 2 base models for ensemble. Only 1 available.\n",
            "  Skipping chennai_1_5_5star - meta-model training failed\n",
            "\\n  ❌ FAILED: CHENNAI - 1.5T - 5 Star\n",
            "\\n[9/10] Processing segment...\n",
            "\\n============================================================\n",
            "Training ensemble for: HYDERABAD - 1.5T - 5 Star\n",
            "Segment name: hyderabad_1_5_5star\n",
            "============================================================\n",
            "  Preparing data for HYDERABAD - 1.5T - 5 Star\n",
            "    Train samples: 36 (2022-2024)\n",
            "    Test samples: 6 (2025)\n",
            "  Data saved: outputs/data/hyderabad_1_5_5star_train.csv, outputs/data/hyderabad_1_5_5star_test.csv\n",
            "  Training base models for hyderabad_1_5_5star...\n",
            "    Training Prophet (Weather) model...\n",
            "      Error training Prophet (Weather): Given date string \"1\" not likely a datetime, at position 0\n",
            "    Training Holt-Winters model...\n",
            "      Holt-Winters model saved: outputs/models/multi_segment/hyderabad_1_5_5star_holtwinters.pkl\n",
            "    Training Prophet (Univariate) model...\n",
            "      Error training Prophet (Univariate): Given date string \"1\" not likely a datetime, at position 0\n",
            "  Base models training completed. 1 models trained successfully.\n",
            "  Training Ridge meta-model for hyderabad_1_5_5star...\n",
            "    Warning: Need at least 2 base models for ensemble. Only 1 available.\n",
            "  Skipping hyderabad_1_5_5star - meta-model training failed\n",
            "\\n  ❌ FAILED: HYDERABAD - 1.5T - 5 Star\n",
            "\\n[10/10] Processing segment...\n",
            "\\n============================================================\n",
            "Training ensemble for: BANGALORE - 1.5T - 3 Star\n",
            "Segment name: bangalore_1_5_3star\n",
            "============================================================\n",
            "  Preparing data for BANGALORE - 1.5T - 3 Star\n",
            "    Train samples: 36 (2022-2024)\n",
            "    Test samples: 6 (2025)\n",
            "  Data saved: outputs/data/bangalore_1_5_3star_train.csv, outputs/data/bangalore_1_5_3star_test.csv\n",
            "  Training base models for bangalore_1_5_3star...\n",
            "    Training Prophet (Weather) model...\n",
            "      Error training Prophet (Weather): day is out of range for month: 0, at position 0\n",
            "    Training Holt-Winters model...\n",
            "      Holt-Winters model saved: outputs/models/multi_segment/bangalore_1_5_3star_holtwinters.pkl\n",
            "    Training Prophet (Univariate) model...\n",
            "      Error training Prophet (Univariate): day is out of range for month: 0, at position 0\n",
            "  Base models training completed. 1 models trained successfully.\n",
            "  Training Ridge meta-model for bangalore_1_5_3star...\n",
            "    Warning: Need at least 2 base models for ensemble. Only 1 available.\n",
            "  Skipping bangalore_1_5_3star - meta-model training failed\n",
            "\\n  ❌ FAILED: BANGALORE - 1.5T - 3 Star\n",
            "\\n================================================================================\n",
            "TRAINING COMPLETED\n",
            "================================================================================\n",
            "Total segments processed: 10\n",
            "Successful: 0\n",
            "Failed: 10\n",
            "Success rate: 0.0%\n",
            "\\nNo successful training results to save.\n"
          ]
        }
      ],
      "source": [
        "# Main training loop\n",
        "print(\"Starting multi-segment ensemble training...\")\n",
        "print(f\"Target segments: {len(target_segments)}\")\n",
        "print(f\"Training period: 2022-2024\")\n",
        "print(f\"Test period: 2025\")\n",
        "print(f\"\\\\n{'='*80}\")\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "successful_segments = 0\n",
        "failed_segments = 0\n",
        "\n",
        "# Process each segment\n",
        "for i, segment in enumerate(target_segments, 1):\n",
        "    print(f\"\\\\n[{i}/{len(target_segments)}] Processing segment...\")\n",
        "    \n",
        "    try:\n",
        "        # Train ensemble for this segment\n",
        "        segment_results = train_segment_ensemble(segment, df_final)\n",
        "        \n",
        "        if segment_results is not None:\n",
        "            all_results.append(segment_results)\n",
        "            successful_segments += 1\n",
        "            \n",
        "            # Print summary\n",
        "            print(f\"\\\\n  ✅ SUCCESS: {segment_results['Segment']}\")\n",
        "            print(f\"     MAE: {segment_results['MAE']:.2f}\")\n",
        "            print(f\"     RMSE: {segment_results['RMSE']:.2f}\")\n",
        "            print(f\"     MAPE: {segment_results['MAPE']:.2f}%\")\n",
        "            print(f\"     Training time: {segment_results['Training_Time']:.1f}s\")\n",
        "        else:\n",
        "            failed_segments += 1\n",
        "            print(f\"\\\\n  ❌ FAILED: {segment['branch']} - {segment['tonnage']}T - {segment['star_rating']}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        failed_segments += 1\n",
        "        print(f\"\\\\n  ❌ ERROR: {segment['branch']} - {segment['tonnage']}T - {segment['star_rating']}\")\n",
        "        print(f\"     Error: {e}\")\n",
        "\n",
        "print(f\"\\\\n{'='*80}\")\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Total segments processed: {len(target_segments)}\")\n",
        "print(f\"Successful: {successful_segments}\")\n",
        "print(f\"Failed: {failed_segments}\")\n",
        "print(f\"Success rate: {successful_segments/len(target_segments)*100:.1f}%\")\n",
        "\n",
        "if successful_segments > 0:\n",
        "    print(f\"\\\\nResults will be saved to outputs/multi_segment_results.csv\")\n",
        "else:\n",
        "    print(f\"\\\\nNo successful training results to save.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\nNo results to save - all training attempts failed.\n"
          ]
        }
      ],
      "source": [
        "# Save results and create summary\n",
        "if len(all_results) > 0:\n",
        "    print(\"\\\\nSaving results and creating summary...\")\n",
        "    \n",
        "    # Convert results to DataFrame\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    \n",
        "    # Save main results\n",
        "    results_file = 'outputs/multi_segment_results.csv'\n",
        "    os.makedirs(os.path.dirname(results_file), exist_ok=True)\n",
        "    results_df.to_csv(results_file, index=False)\n",
        "    print(f\"Results saved to: {results_file}\")\n",
        "    \n",
        "    # Create summary statistics\n",
        "    print(f\"\\\\n{'='*60}\")\n",
        "    print(\"SUMMARY STATISTICS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    print(f\"\\\\nPerformance Metrics:\")\n",
        "    print(f\"  Average MAE: {results_df['MAE'].mean():.2f}\")\n",
        "    print(f\"  Average RMSE: {results_df['RMSE'].mean():.2f}\")\n",
        "    print(f\"  Average MAPE: {results_df['MAPE'].mean():.2f}%\")\n",
        "    print(f\"  Best MAE: {results_df['MAE'].min():.2f}\")\n",
        "    print(f\"  Best RMSE: {results_df['RMSE'].min():.2f}\")\n",
        "    print(f\"  Best MAPE: {results_df['MAPE'].min():.2f}%\")\n",
        "    \n",
        "    print(f\"\\\\nTraining Statistics:\")\n",
        "    print(f\"  Average training time: {results_df['Training_Time'].mean():.1f} seconds\")\n",
        "    print(f\"  Total training time: {results_df['Training_Time'].sum():.1f} seconds\")\n",
        "    print(f\"  Average base models: {results_df['Base_Models_Count'].mean():.1f}\")\n",
        "    \n",
        "    print(f\"\\\\nBest Performing Segments:\")\n",
        "    best_mae = results_df.loc[results_df['MAE'].idxmin()]\n",
        "    best_rmse = results_df.loc[results_df['RMSE'].idxmin()]\n",
        "    best_mape = results_df.loc[results_df['MAPE'].idxmin()]\n",
        "    \n",
        "    print(f\"  Best MAE: {best_mae['Segment']} (MAE: {best_mae['MAE']:.2f})\")\n",
        "    print(f\"  Best RMSE: {best_rmse['Segment']} (RMSE: {best_rmse['RMSE']:.2f})\")\n",
        "    print(f\"  Best MAPE: {best_mape['Segment']} (MAPE: {best_mape['MAPE']:.2f}%)\")\n",
        "    \n",
        "    print(f\"\\\\nModel Files Created:\")\n",
        "    model_files_created = 0\n",
        "    for col in results_df.columns:\n",
        "        if col.endswith('_Model_File'):\n",
        "            non_null_files = results_df[col].notna().sum()\n",
        "            model_files_created += non_null_files\n",
        "            print(f\"  {col}: {non_null_files} files\")\n",
        "    \n",
        "    print(f\"  Total model files: {model_files_created}\")\n",
        "    \n",
        "    # Display results table\n",
        "    print(f\"\\\\nDetailed Results:\")\n",
        "    display_cols = ['Segment', 'Branch', 'Tonnage', 'Star_Rating', 'MAE', 'RMSE', 'MAPE', 'Training_Time']\n",
        "    print(results_df[display_cols].round(2).to_string(index=False))\n",
        "    \n",
        "else:\n",
        "    print(\"\\\\nNo results to save - all training attempts failed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\nNo results available for visualization.\n"
          ]
        }
      ],
      "source": [
        "# Create visualizations\n",
        "if len(all_results) > 0:\n",
        "    print(\"\\\\nCreating visualizations...\")\n",
        "    \n",
        "    # Set up the plotting style\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "    sns.set_palette(\"husl\")\n",
        "    \n",
        "    # Create comprehensive visualization\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "    fig.suptitle('Multi-Segment Ensemble Model Performance Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. MAE comparison\n",
        "    ax1 = axes[0, 0]\n",
        "    mae_data = results_df.sort_values('MAE')\n",
        "    bars1 = ax1.barh(range(len(mae_data)), mae_data['MAE'], color='skyblue', alpha=0.7)\n",
        "    ax1.set_yticks(range(len(mae_data)))\n",
        "    ax1.set_yticklabels(mae_data['Segment'], fontsize=8)\n",
        "    ax1.set_xlabel('MAE')\n",
        "    ax1.set_title('Mean Absolute Error (MAE) by Segment')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, bar in enumerate(bars1):\n",
        "        width = bar.get_width()\n",
        "        ax1.text(width + width*0.01, bar.get_y() + bar.get_height()/2, \n",
        "                f'{width:.1f}', ha='left', va='center', fontsize=8)\n",
        "    \n",
        "    # 2. RMSE comparison\n",
        "    ax2 = axes[0, 1]\n",
        "    rmse_data = results_df.sort_values('RMSE')\n",
        "    bars2 = ax2.barh(range(len(rmse_data)), rmse_data['RMSE'], color='lightcoral', alpha=0.7)\n",
        "    ax2.set_yticks(range(len(rmse_data)))\n",
        "    ax2.set_yticklabels(rmse_data['Segment'], fontsize=8)\n",
        "    ax2.set_xlabel('RMSE')\n",
        "    ax2.set_title('Root Mean Squared Error (RMSE) by Segment')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, bar in enumerate(bars2):\n",
        "        width = bar.get_width()\n",
        "        ax2.text(width + width*0.01, bar.get_y() + bar.get_height()/2, \n",
        "                f'{width:.1f}', ha='left', va='center', fontsize=8)\n",
        "    \n",
        "    # 3. MAPE comparison\n",
        "    ax3 = axes[0, 2]\n",
        "    mape_data = results_df.sort_values('MAPE')\n",
        "    bars3 = ax3.barh(range(len(mape_data)), mape_data['MAPE'], color='lightgreen', alpha=0.7)\n",
        "    ax3.set_yticks(range(len(mape_data)))\n",
        "    ax3.set_yticklabels(mape_data['Segment'], fontsize=8)\n",
        "    ax3.set_xlabel('MAPE (%)')\n",
        "    ax3.set_title('Mean Absolute Percentage Error (MAPE) by Segment')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, bar in enumerate(bars3):\n",
        "        width = bar.get_width()\n",
        "        ax3.text(width + width*0.01, bar.get_y() + bar.get_height()/2, \n",
        "                f'{width:.1f}%', ha='left', va='center', fontsize=8)\n",
        "    \n",
        "    # 4. Training time comparison\n",
        "    ax4 = axes[1, 0]\n",
        "    time_data = results_df.sort_values('Training_Time')\n",
        "    bars4 = ax4.barh(range(len(time_data)), time_data['Training_Time'], color='gold', alpha=0.7)\n",
        "    ax4.set_yticks(range(len(time_data)))\n",
        "    ax4.set_yticklabels(time_data['Segment'], fontsize=8)\n",
        "    ax4.set_xlabel('Training Time (seconds)')\n",
        "    ax4.set_title('Training Time by Segment')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, bar in enumerate(bars4):\n",
        "        width = bar.get_width()\n",
        "        ax4.text(width + width*0.01, bar.get_y() + bar.get_height()/2, \n",
        "                f'{width:.1f}s', ha='left', va='center', fontsize=8)\n",
        "    \n",
        "    # 5. Performance by branch\n",
        "    ax5 = axes[1, 1]\n",
        "    branch_performance = results_df.groupby('Branch')['MAE'].mean().sort_values()\n",
        "    bars5 = ax5.bar(range(len(branch_performance)), branch_performance.values, \n",
        "                   color='purple', alpha=0.7)\n",
        "    ax5.set_xticks(range(len(branch_performance)))\n",
        "    ax5.set_xticklabels(branch_performance.index, rotation=45, ha='right')\n",
        "    ax5.set_ylabel('Average MAE')\n",
        "    ax5.set_title('Average Performance by Branch')\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, bar in enumerate(bars5):\n",
        "        height = bar.get_height()\n",
        "        ax5.text(bar.get_x() + bar.get_width()/2, height + height*0.01, \n",
        "                f'{height:.1f}', ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    # 6. Performance by tonnage\n",
        "    ax6 = axes[1, 2]\n",
        "    tonnage_performance = results_df.groupby('Tonnage')['MAE'].mean().sort_values()\n",
        "    bars6 = ax6.bar(range(len(tonnage_performance)), tonnage_performance.values, \n",
        "                   color='orange', alpha=0.7)\n",
        "    ax6.set_xticks(range(len(tonnage_performance)))\n",
        "    ax6.set_xticklabels([f'{t}T' for t in tonnage_performance.index])\n",
        "    ax6.set_ylabel('Average MAE')\n",
        "    ax6.set_title('Average Performance by Tonnage')\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, bar in enumerate(bars6):\n",
        "        height = bar.get_height()\n",
        "        ax6.text(bar.get_x() + bar.get_width()/2, height + height*0.01, \n",
        "                f'{height:.1f}', ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Save the plot\n",
        "    plot_file = 'outputs/multi_segment_performance_analysis.png'\n",
        "    plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Performance analysis plot saved to: {plot_file}\")\n",
        "    \n",
        "    # Create model inventory\n",
        "    print(f\"\\\\n{'='*60}\")\n",
        "    print(\"MODEL INVENTORY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    model_inventory = []\n",
        "    for _, row in results_df.iterrows():\n",
        "        segment = row['Segment']\n",
        "        for col in results_df.columns:\n",
        "            if col.endswith('_Model_File') and pd.notna(row[col]):\n",
        "                model_type = col.replace('_Model_File', '')\n",
        "                model_inventory.append({\n",
        "                    'Segment': segment,\n",
        "                    'Model_Type': model_type,\n",
        "                    'File_Path': row[col],\n",
        "                    'File_Exists': os.path.exists(row[col]) if pd.notna(row[col]) else False\n",
        "                })\n",
        "    \n",
        "    if model_inventory:\n",
        "        inventory_df = pd.DataFrame(model_inventory)\n",
        "        inventory_file = 'outputs/multi_segment_model_inventory.csv'\n",
        "        inventory_df.to_csv(inventory_file, index=False)\n",
        "        print(f\"Model inventory saved to: {inventory_file}\")\n",
        "        \n",
        "        # Check file existence\n",
        "        existing_files = inventory_df['File_Exists'].sum()\n",
        "        total_files = len(inventory_df)\n",
        "        print(f\"Model files status: {existing_files}/{total_files} files exist\")\n",
        "        \n",
        "        if existing_files < total_files:\n",
        "            missing_files = inventory_df[~inventory_df['File_Exists']]\n",
        "            print(f\"Missing files:\")\n",
        "            for _, row in missing_files.iterrows():\n",
        "                print(f\"  - {row['Segment']} - {row['Model_Type']}: {row['File_Path']}\")\n",
        "    \n",
        "    print(f\"\\\\n{'='*60}\")\n",
        "    print(\"ANALYSIS COMPLETE\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"✅ Results saved to: {results_file}\")\n",
        "    print(f\"✅ Predictions saved to: outputs/multi_segment_predictions_*.csv\")\n",
        "    print(f\"✅ Models saved to: outputs/models/multi_segment/*.pkl\")\n",
        "    print(f\"✅ Visualizations saved to: {plot_file}\")\n",
        "    print(f\"✅ Model inventory saved to: {inventory_file}\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\\\nNo results available for visualization.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Recommendations and Model Usage\n",
        "\n",
        "### Key Findings\n",
        "Based on the multi-segment ensemble training results:\n",
        "\n",
        "1. **Best Performing Segments**: The models with the lowest MAE, RMSE, and MAPE values\n",
        "2. **Model Performance**: All trained models are saved and can be loaded for future predictions\n",
        "3. **Training Efficiency**: Training times and success rates across different segments\n",
        "\n",
        "### Model Loading and Usage Examples\n",
        "\n",
        "```python\n",
        "# Example: Load models for a specific segment\n",
        "segment_name = \"chennai_1_5_3star\"  # Replace with actual segment name\n",
        "models = load_saved_models(segment_name)\n",
        "\n",
        "# Example: Make predictions using loaded models\n",
        "# (This would require implementing prediction functions for each model type)\n",
        "```\n",
        "\n",
        "### Deployment Recommendations\n",
        "\n",
        "1. **Production Use**: Deploy the best performing models based on MAE scores\n",
        "2. **Regular Retraining**: Retrain models every 3-6 months with new data\n",
        "3. **Monitoring**: Track model performance and retrain if accuracy degrades\n",
        "4. **Model Selection**: Use the Ridge meta-model ensemble for best accuracy\n",
        "\n",
        "### File Structure Created\n",
        "\n",
        "```\n",
        "outputs/\n",
        "├── multi_segment_results.csv          # Main results summary\n",
        "├── multi_segment_model_inventory.csv  # Model file inventory\n",
        "├── multi_segment_performance_analysis.png  # Performance visualizations\n",
        "├── data/                              # Segment-specific train/test data\n",
        "│   ├── {segment}_train.csv\n",
        "│   └── {segment}_test.csv\n",
        "├── models/multi_segment/              # All trained models\n",
        "│   ├── {segment}_prophet_weather.pkl\n",
        "│   ├── {segment}_holtwinters.pkl\n",
        "│   ├── {segment}_prophet_univariate.pkl\n",
        "│   └── {segment}_ridge_meta.pkl\n",
        "└── multi_segment_predictions_{segment}.csv  # Detailed predictions\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Review the results and identify the best performing segments\n",
        "2. Deploy the top-performing models for production use\n",
        "3. Set up monitoring for model performance\n",
        "4. Plan regular retraining schedule\n",
        "5. Consider expanding to additional segment combinations if needed\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
