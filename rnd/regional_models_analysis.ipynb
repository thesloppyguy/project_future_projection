{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 5: Regional Models Analysis\n",
        "\n",
        "This notebook implements regional-level forecasting models for each state with two approaches:\n",
        "- **Approach A**: Use only that state's weather data\n",
        "- **Approach B**: Use all 5 states' weather data as features\n",
        "\n",
        "## Models Implemented\n",
        "- ARIMA (univariate)\n",
        "- SARIMAX (with weather regressors)\n",
        "- Seasonal Decomposition\n",
        "- Holt-Winters Exponential Smoothing\n",
        "- LSTM Neural Network\n",
        "- GRU Neural Network\n",
        "- Prophet (with weather regressors)\n",
        "\n",
        "## Training/Validation Split\n",
        "- **Training**: 2021-11 to 2024-12 (38 months)\n",
        "- **Validation**: 2025-01 to 2025-06 (6 months)\n",
        "- **Total Models**: 5 states × 6 models × 2 approaches = 60 models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtsa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mholtwinters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExponentialSmoothing\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtsa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseasonal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m STL\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpmdarima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_arima\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprophet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Prophet\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Machine learning\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/cdro/rnd/.venv/lib/python3.11/site-packages/pmdarima/__init__.py:52\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __check_build\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Stuff we want at top-level\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_arima, ARIMA, AutoARIMA, StepwiseContext, decompose\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m acf, autocorr_plot, c, pacf, plot_acf, plot_pacf, \\\n\u001b[32m     54\u001b[39m     tsdisplay\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/cdro/rnd/.venv/lib/python3.11/site-packages/pmdarima/arima/__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapprox\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/cdro/rnd/.venv/lib/python3.11/site-packages/pmdarima/arima/approx.py:9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# R approx function\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m c, check_endog\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_callable\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTYPE\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/cdro/rnd/.venv/lib/python3.11/site-packages/pmdarima/utils/__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetaestimators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/cdro/rnd/.venv/lib/python3.11/site-packages/pmdarima/utils/array.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTYPE\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m C_intgrt_vec\n\u001b[32m     15\u001b[39m __all__ = [\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mas_series\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mc\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mis_iterable\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     23\u001b[39m ]\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mas_series\u001b[39m(x, **kwargs):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpmdarima/utils/_array.pyx:1\u001b[39m, in \u001b[36minit pmdarima.utils._array\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Time series and forecasting\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from pmdarima import auto_arima\n",
        "from prophet import Prophet\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Utilities\n",
        "import joblib\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_regional_data():\n",
        "    \"\"\"\n",
        "    Load weather and sales data for regional analysis\n",
        "    \"\"\"\n",
        "    # Load weather data for all states\n",
        "    weather_data = []\n",
        "    states = ['AP', 'KA', 'KL', 'TL', 'TN']\n",
        "    state_mapping = {\n",
        "        'AP': 'Andhra Pradesh',\n",
        "        'KA': 'Karnataka', \n",
        "        'KL': 'Kerala',\n",
        "        'TL': 'Telangana',\n",
        "        'TN': 'Tamil Nadu'\n",
        "    }\n",
        "    \n",
        "    for state_code in states:\n",
        "        file_path = f'outputs/processed_weather_data/{state_code}_weather_timeseries.csv'\n",
        "        if os.path.exists(file_path):\n",
        "            df = pd.read_csv(file_path)\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "            df['State'] = state_mapping[state_code]\n",
        "            weather_data.append(df)\n",
        "            print(f\"Loaded weather data for {state_mapping[state_code]}: {len(df)} records\")\n",
        "        else:\n",
        "            print(f\"Warning: Weather file not found for {state_code}\")\n",
        "    \n",
        "    # Combine weather data\n",
        "    weather_df = pd.concat(weather_data, ignore_index=True)\n",
        "    weather_df = weather_df.sort_values(['Date', 'State']).reset_index(drop=True)\n",
        "    \n",
        "    # Load sales data\n",
        "    sales_df = pd.read_csv('data/monthly_sales_summary.csv')\n",
        "    sales_df['Date'] = pd.to_datetime(\n",
        "        sales_df['Year'].astype(str) + '-' + \n",
        "        sales_df['Month'].astype(str) + '-01'\n",
        "    )\n",
        "    \n",
        "    # Aggregate to regional level (State-Month)\n",
        "    regional_sales = sales_df.groupby(['Date', 'State']).agg({\n",
        "        'Monthly_Total_Sales': 'sum',\n",
        "        'Number_of_Transactions': 'sum'\n",
        "    }).reset_index()\n",
        "    \n",
        "    print(f\"\\nWeather data shape: {weather_df.shape}\")\n",
        "    print(f\"Regional sales shape: {regional_sales.shape}\")\n",
        "    print(f\"Date range: {weather_df['Date'].min()} to {weather_df['Date'].max()}\")\n",
        "    print(f\"States: {weather_df['State'].unique()}\")\n",
        "    \n",
        "    return weather_df, regional_sales\n",
        "\n",
        "# Load data\n",
        "weather_df, regional_sales = load_regional_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_modeling_data(weather_df, regional_sales):\n",
        "    \"\"\"\n",
        "    Prepare data for regional modeling with train/validation splits\n",
        "    \"\"\"\n",
        "    # Merge weather and sales data\n",
        "    df_regional = regional_sales.merge(\n",
        "        weather_df, \n",
        "        on=['Date', 'State'], \n",
        "        how='inner'\n",
        "    )\n",
        "    \n",
        "    # Sort by date and state\n",
        "    df_regional = df_regional.sort_values(['Date', 'State']).reset_index(drop=True)\n",
        "    \n",
        "    # Define date ranges\n",
        "    train_end = pd.to_datetime('2024-12-01')\n",
        "    val_end = pd.to_datetime('2025-06-01')\n",
        "    \n",
        "    # Split data\n",
        "    train_data = df_regional[df_regional['Date'] <= train_end].copy()\n",
        "    val_data = df_regional[(df_regional['Date'] > train_end) & (df_regional['Date'] <= val_end)].copy()\n",
        "    \n",
        "    print(f\"Training data: {len(train_data)} records ({train_data['Date'].min()} to {train_data['Date'].max()})\")\n",
        "    print(f\"Validation data: {len(val_data)} records ({val_data['Date'].min()} to {val_data['Date'].max()})\")\n",
        "    \n",
        "    # Create state-specific datasets\n",
        "    state_data = {}\n",
        "    states = df_regional['State'].unique()\n",
        "    \n",
        "    for state in states:\n",
        "        state_train = train_data[train_data['State'] == state].sort_values('Date')\n",
        "        state_val = val_data[val_data['State'] == state].sort_values('Date')\n",
        "        \n",
        "        # Prepare time series\n",
        "        train_ts = state_train.set_index('Date')['Monthly_Total_Sales']\n",
        "        val_ts = state_val.set_index('Date')['Monthly_Total_Sales']\n",
        "        \n",
        "        # Prepare weather features\n",
        "        weather_cols = ['Max_Temp', 'Min_Temp', 'Humidity', 'Wind_Speed']\n",
        "        train_weather = state_train.set_index('Date')[weather_cols]\n",
        "        val_weather = state_val.set_index('Date')[weather_cols]\n",
        "        \n",
        "        state_data[state] = {\n",
        "            'train_ts': train_ts,\n",
        "            'val_ts': val_ts,\n",
        "            'train_weather': train_weather,\n",
        "            'val_weather': val_weather,\n",
        "            'train_data': state_train,\n",
        "            'val_data': state_val,\n",
        "            'weather_cols': weather_cols\n",
        "        }\n",
        "        \n",
        "        print(f\"{state}: Train {len(train_ts)} months, Val {len(val_ts)} months\")\n",
        "    \n",
        "    return state_data, weather_df\n",
        "\n",
        "# Prepare modeling data\n",
        "state_data, all_weather_df = prepare_modeling_data(weather_df, regional_sales)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_arima_model(train_ts, val_ts, state_name, approach):\n",
        "    \"\"\"Fit ARIMA model with automatic parameter selection\"\"\"\n",
        "    try:\n",
        "        model = auto_arima(\n",
        "            train_ts, seasonal=True, m=12,\n",
        "            max_p=3, max_q=3, max_P=2, max_Q=2,\n",
        "            suppress_warnings=True, stepwise=True, error_action='ignore'\n",
        "        )\n",
        "        \n",
        "        order = model.order\n",
        "        seasonal_order = model.seasonal_order\n",
        "        \n",
        "        fitted_model = ARIMA(train_ts, order=order, seasonal_order=seasonal_order).fit()\n",
        "        predictions = fitted_model.forecast(steps=len(val_ts))\n",
        "        \n",
        "        # Save model\n",
        "        joblib.dump(fitted_model, f'outputs/models/regional_arima_{state_name.replace(\" \", \"_\")}_approach_{approach}.pkl')\n",
        "        \n",
        "        return predictions, fitted_model\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting ARIMA for {state_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def fit_sarimax_model(train_ts, train_weather, val_ts, val_weather, state_name, approach):\n",
        "    \"\"\"Fit SARIMAX model with weather regressors\"\"\"\n",
        "    try:\n",
        "        model = auto_arima(\n",
        "            train_ts, exogenous=train_weather, seasonal=True, m=12,\n",
        "            max_p=3, max_q=3, max_P=2, max_Q=2,\n",
        "            suppress_warnings=True, stepwise=True, error_action='ignore'\n",
        "        )\n",
        "        \n",
        "        order = model.order\n",
        "        seasonal_order = model.seasonal_order\n",
        "        \n",
        "        fitted_model = SARIMAX(train_ts, exog=train_weather, order=order, seasonal_order=seasonal_order).fit(disp=False)\n",
        "        predictions = fitted_model.forecast(steps=len(val_ts), exog=val_weather)\n",
        "        \n",
        "        # Save model\n",
        "        joblib.dump(fitted_model, f'outputs/models/regional_sarimax_{state_name.replace(\" \", \"_\")}_approach_{approach}.pkl')\n",
        "        \n",
        "        return predictions, fitted_model\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting SARIMAX for {state_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def seasonal_decomposition_forecast(train_ts, val_ts, state_name, approach):\n",
        "    \"\"\"Forecast using seasonal decomposition approach\"\"\"\n",
        "    try:\n",
        "        stl = STL(train_ts, seasonal=13)\n",
        "        decomposition = stl.fit()\n",
        "        \n",
        "        # Forecast trend\n",
        "        trend = decomposition.trend\n",
        "        trend_dates = np.arange(len(trend)).reshape(-1, 1)\n",
        "        trend_model = LinearRegression().fit(trend_dates, trend.values)\n",
        "        \n",
        "        val_trend_dates = np.arange(len(trend), len(trend) + len(val_ts)).reshape(-1, 1)\n",
        "        trend_forecast = trend_model.predict(val_trend_dates)\n",
        "        \n",
        "        # Use last seasonal pattern\n",
        "        seasonal = decomposition.seasonal\n",
        "        last_year_seasonal = seasonal.iloc[-12:].values\n",
        "        seasonal_forecast = np.tile(last_year_seasonal, (len(val_ts) // 12 + 1))[:len(val_ts)]\n",
        "        \n",
        "        forecast = trend_forecast + seasonal_forecast\n",
        "        \n",
        "        # Save model\n",
        "        joblib.dump({\n",
        "            'trend_model': trend_model,\n",
        "            'seasonal_pattern': last_year_seasonal,\n",
        "            'decomposition': decomposition\n",
        "        }, f'outputs/models/regional_seasonal_decomp_{state_name.replace(\" \", \"_\")}_approach_{approach}.pkl')\n",
        "        \n",
        "        return forecast\n",
        "    except Exception as e:\n",
        "        print(f\"Error in seasonal decomposition for {state_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "def holt_winters_forecast(train_ts, val_ts, state_name, approach):\n",
        "    \"\"\"Fit Holt-Winters exponential smoothing model\"\"\"\n",
        "    try:\n",
        "        hw_model = ExponentialSmoothing(\n",
        "            train_ts, trend='add', seasonal='add', seasonal_periods=12\n",
        "        ).fit(optimized=True)\n",
        "        \n",
        "        forecast = hw_model.forecast(steps=len(val_ts))\n",
        "        \n",
        "        # Save model\n",
        "        joblib.dump(hw_model, f'outputs/models/regional_holt_winters_{state_name.replace(\" \", \"_\")}_approach_{approach}.pkl')\n",
        "        \n",
        "        return forecast\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting Holt-Winters for {state_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Model training functions defined successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_lstm_data(train_ts, train_weather, val_ts, val_weather, lookback=12):\n",
        "    \"\"\"Prepare data for LSTM with lookback window\"\"\"\n",
        "    train_combined = pd.concat([train_ts, train_weather], axis=1)\n",
        "    val_combined = pd.concat([val_ts, val_weather], axis=1)\n",
        "    \n",
        "    scaler = MinMaxScaler()\n",
        "    train_scaled = scaler.fit_transform(train_combined)\n",
        "    val_scaled = scaler.transform(val_combined)\n",
        "    \n",
        "    def create_sequences(data, lookback):\n",
        "        X, y = [], []\n",
        "        for i in range(lookback, len(data)):\n",
        "            X.append(data[i-lookback:i])\n",
        "            y.append(data[i, 0])\n",
        "        return np.array(X), np.array(y)\n",
        "    \n",
        "    X_train, y_train = create_sequences(train_scaled, lookback)\n",
        "    X_val, y_val = create_sequences(val_scaled, lookback)\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val, scaler\n",
        "\n",
        "def fit_lstm_model(train_ts, train_weather, val_ts, val_weather, state_name, approach):\n",
        "    \"\"\"Fit LSTM neural network model\"\"\"\n",
        "    try:\n",
        "        X_train, y_train, X_val, y_val, scaler = prepare_lstm_data(\n",
        "            train_ts, train_weather, val_ts, val_weather, lookback=12\n",
        "        )\n",
        "        \n",
        "        if len(X_train) == 0 or len(X_val) == 0:\n",
        "            print(f\"Insufficient data for LSTM training in {state_name}\")\n",
        "            return None\n",
        "        \n",
        "        model = Sequential([\n",
        "            LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "            Dropout(0.2),\n",
        "            LSTM(32, return_sequences=False),\n",
        "            Dropout(0.2),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        \n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        \n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=100, batch_size=16,\n",
        "            callbacks=[early_stopping], verbose=0\n",
        "        )\n",
        "        \n",
        "        lstm_pred = model.predict(X_val, verbose=0).flatten()\n",
        "        \n",
        "        # Inverse transform predictions\n",
        "        dummy_array = np.zeros((len(lstm_pred), X_train.shape[2]))\n",
        "        dummy_array[:, 0] = lstm_pred\n",
        "        lstm_pred_original = scaler.inverse_transform(dummy_array)[:, 0]\n",
        "        \n",
        "        # Save model and scaler\n",
        "        model.save(f'outputs/models/regional_lstm_{state_name.replace(\" \", \"_\")}_approach_{approach}.h5')\n",
        "        joblib.dump(scaler, f'outputs/models/regional_lstm_scaler_{state_name.replace(\" \", \"_\")}_approach_{approach}.pkl')\n",
        "        \n",
        "        return lstm_pred_original\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting LSTM for {state_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "def fit_gru_model(train_ts, train_weather, val_ts, val_weather, state_name, approach):\n",
        "    \"\"\"Fit GRU neural network model\"\"\"\n",
        "    try:\n",
        "        X_train, y_train, X_val, y_val, scaler = prepare_lstm_data(\n",
        "            train_ts, train_weather, val_ts, val_weather, lookback=12\n",
        "        )\n",
        "        \n",
        "        if len(X_train) == 0 or len(X_val) == 0:\n",
        "            print(f\"Insufficient data for GRU training in {state_name}\")\n",
        "            return None\n",
        "        \n",
        "        model = Sequential([\n",
        "            GRU(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "            Dropout(0.2),\n",
        "            GRU(32, return_sequences=False),\n",
        "            Dropout(0.2),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        \n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        \n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=100, batch_size=16,\n",
        "            callbacks=[early_stopping], verbose=0\n",
        "        )\n",
        "        \n",
        "        gru_pred = model.predict(X_val, verbose=0).flatten()\n",
        "        \n",
        "        # Inverse transform predictions\n",
        "        dummy_array = np.zeros((len(gru_pred), X_train.shape[2]))\n",
        "        dummy_array[:, 0] = gru_pred\n",
        "        gru_pred_original = scaler.inverse_transform(dummy_array)[:, 0]\n",
        "        \n",
        "        # Save model and scaler\n",
        "        model.save(f'outputs/models/regional_gru_{state_name.replace(\" \", \"_\")}_approach_{approach}.h5')\n",
        "        joblib.dump(scaler, f'outputs/models/regional_gru_scaler_{state_name.replace(\" \", \"_\")}_approach_{approach}.pkl')\n",
        "        \n",
        "        return gru_pred_original\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting GRU for {state_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "def fit_prophet_model(train_data, val_data, state_name, approach):\n",
        "    \"\"\"Fit Prophet model with weather regressors\"\"\"\n",
        "    try:\n",
        "        weather_cols = ['Max_Temp', 'Min_Temp', 'Humidity', 'Wind_Speed']\n",
        "        prophet_train = train_data[['Date', 'Monthly_Total_Sales'] + weather_cols].copy()\n",
        "        prophet_train.columns = ['ds', 'y'] + weather_cols\n",
        "        \n",
        "        model = Prophet(\n",
        "            yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False,\n",
        "            seasonality_mode='additive'\n",
        "        )\n",
        "        \n",
        "        for col in weather_cols:\n",
        "            model.add_regressor(col)\n",
        "        \n",
        "        model.fit(prophet_train)\n",
        "        \n",
        "        prophet_val = val_data[['Date'] + weather_cols].copy()\n",
        "        prophet_val.columns = ['ds'] + weather_cols\n",
        "        \n",
        "        forecast = model.predict(prophet_val)\n",
        "        prophet_pred = forecast['yhat'].values\n",
        "        \n",
        "        # Save model\n",
        "        joblib.dump(model, f'outputs/models/regional_prophet_{state_name.replace(\" \", \"_\")}_approach_{approach}.pkl')\n",
        "        \n",
        "        return prophet_pred\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting Prophet for {state_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Neural network and Prophet functions defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Approach A: Own Weather Data Only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all models for Approach A (own weather data only)\n",
        "print(\"Training Approach A models (own weather data only)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "approach_a_results = {}\n",
        "\n",
        "for state in state_data.keys():\n",
        "    print(f\"\\nTraining models for {state}...\")\n",
        "    \n",
        "    data = state_data[state]\n",
        "    train_ts = data['train_ts']\n",
        "    val_ts = data['val_ts']\n",
        "    train_weather = data['train_weather']\n",
        "    val_weather = data['val_weather']\n",
        "    train_data = data['train_data']\n",
        "    val_data = data['val_data']\n",
        "    \n",
        "    state_results = {}\n",
        "    \n",
        "    # Train all 6 models\n",
        "    print(f\"  - ARIMA...\")\n",
        "    arima_pred, _ = fit_arima_model(train_ts, val_ts, state, 'A')\n",
        "    state_results['ARIMA'] = arima_pred\n",
        "    \n",
        "    print(f\"  - SARIMAX...\")\n",
        "    sarimax_pred, _ = fit_sarimax_model(train_ts, train_weather, val_ts, val_weather, state, 'A')\n",
        "    state_results['SARIMAX'] = sarimax_pred\n",
        "    \n",
        "    print(f\"  - Seasonal Decomposition...\")\n",
        "    decomp_pred = seasonal_decomposition_forecast(train_ts, val_ts, state, 'A')\n",
        "    state_results['Seasonal_Decomp'] = decomp_pred\n",
        "    \n",
        "    print(f\"  - Holt-Winters...\")\n",
        "    hw_pred = holt_winters_forecast(train_ts, val_ts, state, 'A')\n",
        "    state_results['Holt_Winters'] = hw_pred\n",
        "    \n",
        "    print(f\"  - LSTM...\")\n",
        "    lstm_pred = fit_lstm_model(train_ts, train_weather, val_ts, val_weather, state, 'A')\n",
        "    state_results['LSTM'] = lstm_pred\n",
        "    \n",
        "    print(f\"  - GRU...\")\n",
        "    gru_pred = fit_gru_model(train_ts, train_weather, val_ts, val_weather, state, 'A')\n",
        "    state_results['GRU'] = gru_pred\n",
        "    \n",
        "    print(f\"  - Prophet...\")\n",
        "    prophet_pred = fit_prophet_model(train_data, val_data, state, 'A')\n",
        "    state_results['Prophet'] = prophet_pred\n",
        "    \n",
        "    approach_a_results[state] = {\n",
        "        'predictions': state_results,\n",
        "        'actual': val_ts.values,\n",
        "        'dates': val_ts.index\n",
        "    }\n",
        "    \n",
        "    print(f\"  ✅ Completed {state}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Approach A training completed!\")\n",
        "print(f\"Models saved to: outputs/models/regional_*_approach_A.*\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Approach B: All States Weather Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_all_states_weather_data(state_data, all_weather_df):\n",
        "    \"\"\"Prepare weather data from all states as features for each state\"\"\"\n",
        "    # Create wide format weather data (all states as columns)\n",
        "    weather_wide = all_weather_df.pivot_table(\n",
        "        index='Date', \n",
        "        columns='State', \n",
        "        values=['Max_Temp', 'Min_Temp', 'Humidity', 'Wind_Speed'],\n",
        "        aggfunc='mean'\n",
        "    )\n",
        "    \n",
        "    # Flatten column names\n",
        "    weather_wide.columns = [f\"{metric}_{state.replace(' ', '_')}\" for metric, state in weather_wide.columns]\n",
        "    weather_wide = weather_wide.reset_index()\n",
        "    \n",
        "    # Create state-specific datasets with all weather features\n",
        "    all_states_data = {}\n",
        "    \n",
        "    for state in state_data.keys():\n",
        "        train_data = state_data[state]['train_data']\n",
        "        val_data = state_data[state]['val_data']\n",
        "        \n",
        "        # Merge with all weather data\n",
        "        train_merged = train_data.merge(weather_wide, on='Date', how='inner')\n",
        "        val_merged = val_data.merge(weather_wide, on='Date', how='inner')\n",
        "        \n",
        "        # Prepare time series\n",
        "        train_ts = train_merged.set_index('Date')['Monthly_Total_Sales']\n",
        "        val_ts = val_merged.set_index('Date')['Monthly_Total_Sales']\n",
        "        \n",
        "        # Prepare all weather features\n",
        "        weather_cols = [col for col in weather_wide.columns if col != 'Date']\n",
        "        train_weather = train_merged.set_index('Date')[weather_cols]\n",
        "        val_weather = val_merged.set_index('Date')[weather_cols]\n",
        "        \n",
        "        all_states_data[state] = {\n",
        "            'train_ts': train_ts,\n",
        "            'val_ts': val_ts,\n",
        "            'train_weather': train_weather,\n",
        "            'val_weather': val_weather,\n",
        "            'train_data': train_merged,\n",
        "            'val_data': val_merged,\n",
        "            'weather_cols': weather_cols\n",
        "        }\n",
        "        \n",
        "        print(f\"{state}: {len(weather_cols)} weather features from all states\")\n",
        "    \n",
        "    return all_states_data\n",
        "\n",
        "# Prepare all states weather data\n",
        "all_states_data = prepare_all_states_weather_data(state_data, all_weather_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all models for Approach B (all states weather data)\n",
        "print(\"Training Approach B models (all states weather data)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "approach_b_results = {}\n",
        "\n",
        "for state in all_states_data.keys():\n",
        "    print(f\"\\nTraining models for {state}...\")\n",
        "    \n",
        "    data = all_states_data[state]\n",
        "    train_ts = data['train_ts']\n",
        "    val_ts = data['val_ts']\n",
        "    train_weather = data['train_weather']\n",
        "    val_weather = data['val_weather']\n",
        "    train_data = data['train_data']\n",
        "    val_data = data['val_data']\n",
        "    \n",
        "    state_results = {}\n",
        "    \n",
        "    # Train all 6 models (ARIMA and Seasonal Decomp are univariate, same as Approach A)\n",
        "    print(f\"  - ARIMA...\")\n",
        "    arima_pred, _ = fit_arima_model(train_ts, val_ts, state, 'B')\n",
        "    state_results['ARIMA'] = arima_pred\n",
        "    \n",
        "    print(f\"  - SARIMAX...\")\n",
        "    sarimax_pred, _ = fit_sarimax_model(train_ts, train_weather, val_ts, val_weather, state, 'B')\n",
        "    state_results['SARIMAX'] = sarimax_pred\n",
        "    \n",
        "    print(f\"  - Seasonal Decomposition...\")\n",
        "    decomp_pred = seasonal_decomposition_forecast(train_ts, val_ts, state, 'B')\n",
        "    state_results['Seasonal_Decomp'] = decomp_pred\n",
        "    \n",
        "    print(f\"  - Holt-Winters...\")\n",
        "    hw_pred = holt_winters_forecast(train_ts, val_ts, state, 'B')\n",
        "    state_results['Holt_Winters'] = hw_pred\n",
        "    \n",
        "    print(f\"  - LSTM...\")\n",
        "    lstm_pred = fit_lstm_model(train_ts, train_weather, val_ts, val_weather, state, 'B')\n",
        "    state_results['LSTM'] = lstm_pred\n",
        "    \n",
        "    print(f\"  - GRU...\")\n",
        "    gru_pred = fit_gru_model(train_ts, train_weather, val_ts, val_weather, state, 'B')\n",
        "    state_results['GRU'] = gru_pred\n",
        "    \n",
        "    print(f\"  - Prophet...\")\n",
        "    prophet_pred = fit_prophet_model(train_data, val_data, state, 'B')\n",
        "    state_results['Prophet'] = prophet_pred\n",
        "    \n",
        "    approach_b_results[state] = {\n",
        "        'predictions': state_results,\n",
        "        'actual': val_ts.values,\n",
        "        'dates': val_ts.index\n",
        "    }\n",
        "    \n",
        "    print(f\"  ✅ Completed {state}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Approach B training completed!\")\n",
        "print(f\"Models saved to: outputs/models/regional_*_approach_B.*\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Evaluation and Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_models(predictions_dict, actual_values, dates, state_name, approach):\n",
        "    \"\"\"Evaluate model performance and return metrics\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for model_name, pred in predictions_dict.items():\n",
        "        if pred is not None and len(pred) == len(actual_values):\n",
        "            # Calculate metrics\n",
        "            mae = np.mean(np.abs(pred - actual_values))\n",
        "            rmse = np.sqrt(np.mean((pred - actual_values) ** 2))\n",
        "            mape = np.mean(np.abs((actual_values - pred) / actual_values)) * 100\n",
        "            \n",
        "            results.append({\n",
        "                'State': state_name,\n",
        "                'Model': model_name,\n",
        "                'Approach': approach,\n",
        "                'MAE': mae,\n",
        "                'RMSE': rmse,\n",
        "                'MAPE': mape\n",
        "            })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Evaluate all models\n",
        "all_results = []\n",
        "\n",
        "print(\"Evaluating Approach A models...\")\n",
        "for state, data in approach_a_results.items():\n",
        "    results = evaluate_models(\n",
        "        data['predictions'], \n",
        "        data['actual'], \n",
        "        data['dates'], \n",
        "        state, \n",
        "        'A'\n",
        "    )\n",
        "    all_results.extend(results)\n",
        "\n",
        "print(\"Evaluating Approach B models...\")\n",
        "for state, data in approach_b_results.items():\n",
        "    results = evaluate_models(\n",
        "        data['predictions'], \n",
        "        data['actual'], \n",
        "        data['dates'], \n",
        "        state, \n",
        "        'B'\n",
        "    )\n",
        "    all_results.extend(results)\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "print(\"\\nModel Performance Summary:\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.groupby(['State', 'Approach', 'Model'])[['MAE', 'RMSE', 'MAPE']].mean().round(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive comparison visualizations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "fig.suptitle('Regional Models Performance Comparison: Approach A vs B', fontsize=16, y=0.98)\n",
        "\n",
        "# 1. MAE comparison by state\n",
        "mae_pivot = results_df.pivot_table(index=['State', 'Model'], columns='Approach', values='MAE')\n",
        "mae_pivot.plot(kind='bar', ax=axes[0, 0], width=0.8)\n",
        "axes[0, 0].set_title('MAE Comparison by State and Model')\n",
        "axes[0, 0].set_ylabel('MAE')\n",
        "axes[0, 0].legend(['Approach A (Own Weather)', 'Approach B (All Weather)'])\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 2. RMSE comparison by state\n",
        "rmse_pivot = results_df.pivot_table(index=['State', 'Model'], columns='Approach', values='RMSE')\n",
        "rmse_pivot.plot(kind='bar', ax=axes[0, 1], width=0.8)\n",
        "axes[0, 1].set_title('RMSE Comparison by State and Model')\n",
        "axes[0, 1].set_ylabel('RMSE')\n",
        "axes[0, 1].legend(['Approach A (Own Weather)', 'Approach B (All Weather)'])\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. MAPE comparison by state\n",
        "mape_pivot = results_df.pivot_table(index=['State', 'Model'], columns='Approach', values='MAPE')\n",
        "mape_pivot.plot(kind='bar', ax=axes[0, 2], width=0.8)\n",
        "axes[0, 2].set_title('MAPE Comparison by State and Model')\n",
        "axes[0, 2].set_ylabel('MAPE (%)')\n",
        "axes[0, 2].legend(['Approach A (Own Weather)', 'Approach B (All Weather)'])\n",
        "axes[0, 2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 4. Best model per state (by MAE)\n",
        "best_models = results_df.loc[results_df.groupby('State')['MAE'].idxmin()]\n",
        "best_models_pivot = best_models.pivot_table(index='State', columns='Approach', values='Model', aggfunc='first')\n",
        "best_models_pivot.plot(kind='bar', ax=axes[1, 0], width=0.8)\n",
        "axes[1, 0].set_title('Best Model per State (by MAE)')\n",
        "axes[1, 0].set_ylabel('Model')\n",
        "axes[1, 0].legend(['Approach A (Own Weather)', 'Approach B (All Weather)'])\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 5. Average performance by approach\n",
        "avg_performance = results_df.groupby('Approach')[['MAE', 'RMSE', 'MAPE']].mean()\n",
        "avg_performance.plot(kind='bar', ax=axes[1, 1], width=0.8)\n",
        "axes[1, 1].set_title('Average Performance by Approach')\n",
        "axes[1, 1].set_ylabel('Metric Value')\n",
        "axes[1, 1].legend(['MAE', 'RMSE', 'MAPE'])\n",
        "axes[1, 1].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# 6. Model performance heatmap\n",
        "model_performance = results_df.groupby(['Model', 'Approach'])['MAE'].mean().unstack()\n",
        "sns.heatmap(model_performance, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[1, 2])\n",
        "axes[1, 2].set_title('Model Performance Heatmap (MAE)')\n",
        "axes[1, 2].set_xlabel('Approach')\n",
        "axes[1, 2].set_ylabel('Model')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/charts/regional_models_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nComparison chart saved to: outputs/charts/regional_models_comparison.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed performance analysis\n",
        "print(\"\\nDetailed Performance Analysis:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Best approach per state\n",
        "print(\"\\nBest Approach per State (by average MAE):\")\n",
        "state_avg = results_df.groupby(['State', 'Approach'])['MAE'].mean().unstack()\n",
        "best_approach = state_avg.idxmin(axis=1)\n",
        "print(best_approach)\n",
        "\n",
        "# Best model per state\n",
        "print(\"\\nBest Model per State (by MAE):\")\n",
        "best_model_per_state = results_df.loc[results_df.groupby('State')['MAE'].idxmin()]\n",
        "print(best_model_per_state[['State', 'Model', 'Approach', 'MAE', 'RMSE', 'MAPE']])\n",
        "\n",
        "# Overall best models\n",
        "print(\"\\nOverall Best Models (Top 10 by MAE):\")\n",
        "top_models = results_df.nsmallest(10, 'MAE')\n",
        "print(top_models[['State', 'Model', 'Approach', 'MAE', 'RMSE', 'MAPE']])\n",
        "\n",
        "# Approach comparison summary\n",
        "print(\"\\nApproach Comparison Summary:\")\n",
        "approach_summary = results_df.groupby('Approach')[['MAE', 'RMSE', 'MAPE']].agg(['mean', 'std']).round(3)\n",
        "print(approach_summary)\n",
        "\n",
        "# Save detailed results\n",
        "results_df.to_csv('outputs/regional_models_results.csv', index=False)\n",
        "print(\"\\nDetailed results saved to: outputs/regional_models_results.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary and Next Steps\n",
        "\n",
        "### Completed in this notebook:\n",
        "✅ **Data Preparation**: Loaded weather and sales data, created train/validation splits  \n",
        "✅ **Approach A**: Trained 30 models (5 states × 6 models) using only own weather data  \n",
        "✅ **Approach B**: Trained 30 models (5 states × 6 models) using all states' weather data  \n",
        "✅ **Model Evaluation**: Calculated MAE, RMSE, MAPE for all 60 models  \n",
        "✅ **Performance Comparison**: Created comprehensive visualizations comparing approaches  \n",
        "✅ **Model Persistence**: Saved all 60 trained models to `outputs/models/`  \n",
        "\n",
        "### Key Findings:\n",
        "- **Total Models Trained**: 60 (30 Approach A + 30 Approach B)\n",
        "- **Models per State**: ARIMA, SARIMAX, Seasonal Decomp, Holt-Winters, LSTM, GRU, Prophet\n",
        "- **Validation Period**: 2025-01 to 2025-06 (6 months)\n",
        "- **Best Performance**: [See detailed results above]\n",
        "\n",
        "### Next Steps:\n",
        "1. **Phase 6**: Segment-level analysis (`segment_models_analysis.ipynb`)\n",
        "2. **Phase 7**: Forecasting and outputs (`forecasting_and_outputs.ipynb`)\n",
        "3. **Final Integration**: Combine all results and generate comprehensive report\n",
        "\n",
        "### Files Generated:\n",
        "- `outputs/charts/regional_models_comparison.png` - Performance comparison charts\n",
        "- `outputs/regional_models_results.csv` - Detailed performance metrics\n",
        "- `outputs/models/regional_*_approach_A.*` - 30 Approach A models\n",
        "- `outputs/models/regional_*_approach_B.*` - 30 Approach B models\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
