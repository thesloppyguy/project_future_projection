{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Model Forecasting Pipeline for 6 Locations\n",
        "\n",
        "This notebook implements a comprehensive forecasting system that trains and evaluates multiple model families on each location's sales data, using the last 12 months as test data while preventing QTY leakage through proper feature engineering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data processing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Traditional ML models\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Time series models\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from prophet import Prophet\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Nixtla client for TimeGPT\n",
        "from nixtla import NixtlaClient\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (286, 18)\n",
            "Date range: 2019-04-01 to 2024-03-01\n",
            "Locations: ['Bangalore' 'Chennai' 'Cochin' 'Secunderabad' 'Vijaywada' 'Sricity']\n",
            "Columns: ['Location', 'QTY', 'temp_min', 'temp_max', 'temp_mean', 'dwpt_min', 'dwpt_max', 'dwpt_mean', 'rhum_min', 'rhum_max', 'rhum_mean', 'prcp_min', 'prcp_max', 'prcp_mean', 'wspd_min', 'wspd_max', 'wspd_mean', 'date']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Location</th>\n",
              "      <th>QTY</th>\n",
              "      <th>temp_min</th>\n",
              "      <th>temp_max</th>\n",
              "      <th>temp_mean</th>\n",
              "      <th>dwpt_min</th>\n",
              "      <th>dwpt_max</th>\n",
              "      <th>dwpt_mean</th>\n",
              "      <th>rhum_min</th>\n",
              "      <th>rhum_max</th>\n",
              "      <th>rhum_mean</th>\n",
              "      <th>prcp_min</th>\n",
              "      <th>prcp_max</th>\n",
              "      <th>prcp_mean</th>\n",
              "      <th>wspd_min</th>\n",
              "      <th>wspd_max</th>\n",
              "      <th>wspd_mean</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bangalore</td>\n",
              "      <td>1678.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>34.40</td>\n",
              "      <td>25.490457</td>\n",
              "      <td>-2.240</td>\n",
              "      <td>21.820</td>\n",
              "      <td>12.241344</td>\n",
              "      <td>11.20</td>\n",
              "      <td>99.6</td>\n",
              "      <td>49.995161</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.875000</td>\n",
              "      <td>0.009845</td>\n",
              "      <td>1.08</td>\n",
              "      <td>34.480</td>\n",
              "      <td>10.858306</td>\n",
              "      <td>2019-04-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bangalore</td>\n",
              "      <td>3951.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>35.25</td>\n",
              "      <td>27.041071</td>\n",
              "      <td>0.775</td>\n",
              "      <td>23.275</td>\n",
              "      <td>15.193029</td>\n",
              "      <td>12.25</td>\n",
              "      <td>100.0</td>\n",
              "      <td>54.193651</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.766667</td>\n",
              "      <td>0.056111</td>\n",
              "      <td>0.00</td>\n",
              "      <td>36.625</td>\n",
              "      <td>10.347719</td>\n",
              "      <td>2019-05-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bangalore</td>\n",
              "      <td>4709.0</td>\n",
              "      <td>19.6</td>\n",
              "      <td>34.40</td>\n",
              "      <td>26.270377</td>\n",
              "      <td>10.240</td>\n",
              "      <td>24.420</td>\n",
              "      <td>19.395948</td>\n",
              "      <td>24.80</td>\n",
              "      <td>100.0</td>\n",
              "      <td>69.900511</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.166667</td>\n",
              "      <td>0.159319</td>\n",
              "      <td>0.00</td>\n",
              "      <td>38.960</td>\n",
              "      <td>13.101580</td>\n",
              "      <td>2019-06-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bangalore</td>\n",
              "      <td>2418.0</td>\n",
              "      <td>19.6</td>\n",
              "      <td>32.40</td>\n",
              "      <td>24.810296</td>\n",
              "      <td>14.280</td>\n",
              "      <td>24.020</td>\n",
              "      <td>19.992738</td>\n",
              "      <td>38.00</td>\n",
              "      <td>100.0</td>\n",
              "      <td>76.857086</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.800000</td>\n",
              "      <td>0.155787</td>\n",
              "      <td>1.44</td>\n",
              "      <td>41.540</td>\n",
              "      <td>17.832391</td>\n",
              "      <td>2019-07-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bangalore</td>\n",
              "      <td>2252.0</td>\n",
              "      <td>19.6</td>\n",
              "      <td>30.40</td>\n",
              "      <td>23.654283</td>\n",
              "      <td>16.740</td>\n",
              "      <td>23.260</td>\n",
              "      <td>20.154345</td>\n",
              "      <td>45.40</td>\n",
              "      <td>100.0</td>\n",
              "      <td>82.423583</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.633333</td>\n",
              "      <td>0.201703</td>\n",
              "      <td>1.44</td>\n",
              "      <td>41.600</td>\n",
              "      <td>18.792103</td>\n",
              "      <td>2019-08-01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Location     QTY  temp_min  temp_max  temp_mean  dwpt_min  dwpt_max  \\\n",
              "0  Bangalore  1678.0      15.0     34.40  25.490457    -2.240    21.820   \n",
              "1  Bangalore  3951.0      19.0     35.25  27.041071     0.775    23.275   \n",
              "2  Bangalore  4709.0      19.6     34.40  26.270377    10.240    24.420   \n",
              "3  Bangalore  2418.0      19.6     32.40  24.810296    14.280    24.020   \n",
              "4  Bangalore  2252.0      19.6     30.40  23.654283    16.740    23.260   \n",
              "\n",
              "   dwpt_mean  rhum_min  rhum_max  rhum_mean  prcp_min  prcp_max  prcp_mean  \\\n",
              "0  12.241344     11.20      99.6  49.995161       0.0  1.875000   0.009845   \n",
              "1  15.193029     12.25     100.0  54.193651       0.0  7.766667   0.056111   \n",
              "2  19.395948     24.80     100.0  69.900511       0.0  8.166667   0.159319   \n",
              "3  19.992738     38.00     100.0  76.857086       0.0  7.800000   0.155787   \n",
              "4  20.154345     45.40     100.0  82.423583       0.0  7.633333   0.201703   \n",
              "\n",
              "   wspd_min  wspd_max  wspd_mean       date  \n",
              "0      1.08    34.480  10.858306 2019-04-01  \n",
              "1      0.00    36.625  10.347719 2019-05-01  \n",
              "2      0.00    38.960  13.101580 2019-06-01  \n",
              "3      1.44    41.540  17.832391 2019-07-01  \n",
              "4      1.44    41.600  18.792103 2019-08-01  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the consolidated data\n",
        "df = pd.read_csv('../outputs/data/sales_weather_merged_filled_consolidated.csv')\n",
        "print(f\"Data shape: {df.shape}\")\n",
        "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "print(f\"Locations: {df['Location'].unique()}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Convert date column to datetime\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.sort_values(['Location', 'date']).reset_index(drop=True)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time feature functions created successfully!\n"
          ]
        }
      ],
      "source": [
        "def create_time_features(df):\n",
        "    \"\"\"\n",
        "    Create comprehensive time-based features for forecasting\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with 'date' column and other features\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with additional time features\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Extract temporal indicators\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['quarter'] = df['date'].dt.quarter\n",
        "    df['day_of_year'] = df['date'].dt.dayofyear\n",
        "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
        "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
        "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
        "    df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n",
        "    df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)\n",
        "    \n",
        "    # Cyclical encoding for seasonality\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "    df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)\n",
        "    df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)\n",
        "    \n",
        "    # Weather interaction features\n",
        "    if 'temp_mean' in df.columns and 'rhum_mean' in df.columns:\n",
        "        df['temp_humidity_interaction'] = df['temp_mean'] * df['rhum_mean']\n",
        "    if 'temp_mean' in df.columns and 'wspd_mean' in df.columns:\n",
        "        df['temp_wind_interaction'] = df['temp_mean'] * df['wspd_mean']\n",
        "    \n",
        "    return df\n",
        "\n",
        "def create_lag_and_rolling_features(df, target_col='QTY', lags=[1, 2, 3, 6, 12], windows=[3, 6, 12]):\n",
        "    \"\"\"\n",
        "    Create lag and rolling features to prevent data leakage\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame sorted by date\n",
        "        target_col: Target column name\n",
        "        lags: List of lag periods\n",
        "        windows: List of rolling window sizes\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with lag and rolling features\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Create lag features (only using past data)\n",
        "    for lag in lags:\n",
        "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
        "    \n",
        "    # Create rolling statistics (only using past data)\n",
        "    for window in windows:\n",
        "        df[f'{target_col}_rolling_mean_{window}'] = df[target_col].rolling(window=window, min_periods=1).mean().shift(1)\n",
        "        df[f'{target_col}_rolling_std_{window}'] = df[target_col].rolling(window=window, min_periods=1).std().shift(1)\n",
        "        df[f'{target_col}_rolling_min_{window}'] = df[target_col].rolling(window=window, min_periods=1).min().shift(1)\n",
        "        df[f'{target_col}_rolling_max_{window}'] = df[target_col].rolling(window=window, min_periods=1).max().shift(1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"Time feature functions created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data preparation function created successfully!\n"
          ]
        }
      ],
      "source": [
        "def prepare_location_data(df, location, test_months=12):\n",
        "    \"\"\"\n",
        "    Prepare data for a specific location with train/test split\n",
        "    \n",
        "    Args:\n",
        "        df: Full dataset\n",
        "        location: Location name\n",
        "        test_months: Number of months to use for testing\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with train/test data and scalers\n",
        "    \"\"\"\n",
        "    # Filter data for specific location\n",
        "    location_df = df[df['Location'] == location].copy()\n",
        "    location_df = location_df.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    # Check if we have enough data\n",
        "    if len(location_df) < test_months + 12:  # Need at least 12 months for training + test_months\n",
        "        print(f\"Warning: {location} has insufficient data ({len(location_df)} records). Skipping.\")\n",
        "        return None\n",
        "    \n",
        "    # Create time features\n",
        "    location_df = create_time_features(location_df)\n",
        "    \n",
        "    # Create lag and rolling features\n",
        "    location_df = create_lag_and_rolling_features(location_df)\n",
        "    \n",
        "    # Split data: last test_months for testing\n",
        "    split_idx = len(location_df) - test_months\n",
        "    train_df = location_df.iloc[:split_idx].copy()\n",
        "    test_df = location_df.iloc[split_idx:].copy()\n",
        "    \n",
        "    # Remove rows with NaN from lag/rolling features in training set\n",
        "    train_df = train_df.dropna().reset_index(drop=True)\n",
        "    \n",
        "    # Check if we have any training data left\n",
        "    if len(train_df) == 0:\n",
        "        print(f\"Warning: {location} has no valid training data after feature engineering. Skipping.\")\n",
        "        return None\n",
        "    \n",
        "    # Define feature columns (exclude target and non-feature columns)\n",
        "    exclude_cols = ['QTY', 'date', 'Location']\n",
        "    feature_cols = [col for col in location_df.columns if col not in exclude_cols]\n",
        "    \n",
        "    # Prepare features and target\n",
        "    X_train = train_df[feature_cols].fillna(0)  # Fill any remaining NaN with 0\n",
        "    y_train = train_df['QTY']\n",
        "    X_test = test_df[feature_cols].fillna(0)\n",
        "    y_test = test_df['QTY']\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    return {\n",
        "        'X_train': X_train_scaled,\n",
        "        'X_test': X_test_scaled,\n",
        "        'y_train': y_train.values,\n",
        "        'y_test': y_test.values,\n",
        "        'train_df': train_df,\n",
        "        'test_df': test_df,\n",
        "        'feature_cols': feature_cols,\n",
        "        'scaler': scaler\n",
        "    }\n",
        "\n",
        "print(\"Data preparation function created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Location: Chennai\n",
            "Training samples: 35\n",
            "Test samples: 12\n",
            "Number of features: 47\n",
            "Feature columns: 47\n",
            "\n",
            "Sample feature names:\n",
            "['temp_min', 'temp_max', 'temp_mean', 'dwpt_min', 'dwpt_max', 'dwpt_mean', 'rhum_min', 'rhum_max', 'rhum_mean', 'prcp_min']\n"
          ]
        }
      ],
      "source": [
        "# Test the data preparation for one location\n",
        "test_location = 'Chennai'\n",
        "location_data = prepare_location_data(df, test_location)\n",
        "\n",
        "print(f\"Location: {test_location}\")\n",
        "print(f\"Training samples: {len(location_data['y_train'])}\")\n",
        "print(f\"Test samples: {len(location_data['y_test'])}\")\n",
        "print(f\"Number of features: {location_data['X_train'].shape[1]}\")\n",
        "print(f\"Feature columns: {len(location_data['feature_cols'])}\")\n",
        "\n",
        "# Show sample of features\n",
        "print(\"\\nSample feature names:\")\n",
        "print(location_data['feature_cols'][:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ML models function created successfully!\n"
          ]
        }
      ],
      "source": [
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate evaluation metrics\n",
        "    \"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    return {\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'R2': r2\n",
        "    }\n",
        "\n",
        "def train_ml_models(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate traditional ML models\n",
        "    \"\"\"\n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Ridge': Ridge(alpha=1.0),\n",
        "        'Lasso': Lasso(alpha=0.1),\n",
        "        'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
        "        'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "        'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
        "        'Extra Trees': ExtraTreesRegressor(n_estimators=100, random_state=42),\n",
        "        'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, verbosity=0),\n",
        "        'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, verbosity=-1),\n",
        "        'CatBoost': CatBoostRegressor(iterations=100, random_state=42, verbose=False),\n",
        "        'SVR': SVR(kernel='rbf'),\n",
        "        'KNN': KNeighborsRegressor(n_neighbors=5)\n",
        "    }\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            start_time = datetime.now()\n",
        "            \n",
        "            # Train model\n",
        "            model.fit(X_train, y_train)\n",
        "            \n",
        "            # Make predictions\n",
        "            y_pred = model.predict(X_test)\n",
        "            \n",
        "            # Calculate metrics\n",
        "            metrics = calculate_metrics(y_test, y_pred)\n",
        "            \n",
        "            training_time = (datetime.now() - start_time).total_seconds()\n",
        "            \n",
        "            results.append({\n",
        "                'Model_Type': 'ML',\n",
        "                'Model_Name': name,\n",
        "                'MAE': metrics['MAE'],\n",
        "                'RMSE': metrics['RMSE'],\n",
        "                'MAPE': metrics['MAPE'],\n",
        "                'R2': metrics['R2'],\n",
        "                'Training_Time': training_time,\n",
        "                'Predictions': y_pred\n",
        "            })\n",
        "            \n",
        "            print(f\"✓ {name}: MAE={metrics['MAE']:.2f}, RMSE={metrics['RMSE']:.2f}, R²={metrics['R2']:.3f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ {name}: Failed - {str(e)}\")\n",
        "            \n",
        "    return results\n",
        "\n",
        "print(\"ML models function created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deep learning models functions created successfully!\n"
          ]
        }
      ],
      "source": [
        "def create_sequences(X, y, seq_length=12):\n",
        "    \"\"\"\n",
        "    Create sequences for LSTM/GRU models\n",
        "    \"\"\"\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(seq_length, len(X)):\n",
        "        X_seq.append(X[i-seq_length:i])\n",
        "        y_seq.append(y[i])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "def train_transformer_model(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train Transformer model (simplified version)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # For now, use a simple feed-forward network as transformer substitute\n",
        "        # In a full implementation, you would use a proper transformer architecture\n",
        "        model = Sequential([\n",
        "            Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "            Dense(64, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        \n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        \n",
        "        # Train model\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(y_test, y_pred)\n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        return {\n",
        "            'Model_Type': 'Deep Learning',\n",
        "            'Model_Name': 'Transformer',\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'R2': metrics['R2'],\n",
        "            'Training_Time': training_time,\n",
        "            'Predictions': y_pred\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Transformer failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def train_tcn_model(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train TCN model (simplified version)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # For now, use a simple feed-forward network as TCN substitute\n",
        "        # In a full implementation, you would use a proper TCN architecture\n",
        "        model = Sequential([\n",
        "            Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "            Dense(64, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        \n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        \n",
        "        # Train model\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(y_test, y_pred)\n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        return {\n",
        "            'Model_Type': 'Deep Learning',\n",
        "            'Model_Name': 'TCN',\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'R2': metrics['R2'],\n",
        "            'Training_Time': training_time,\n",
        "            'Predictions': y_pred\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"TCN failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def train_lstm_model(X_train, y_train, X_test, y_test, seq_length=6):\n",
        "    \"\"\"\n",
        "    Train LSTM model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if we have enough data for sequences\n",
        "        if len(X_train) < seq_length + 1 or len(X_test) < seq_length + 1:\n",
        "            print(f\"LSTM: Insufficient data for sequences (train: {len(X_train)}, test: {len(X_test)})\")\n",
        "            return None\n",
        "        \n",
        "        # Create sequences\n",
        "        X_train_seq, y_train_seq = create_sequences(X_train, y_train, seq_length)\n",
        "        X_test_seq, y_test_seq = create_sequences(X_test, y_test, seq_length)\n",
        "        \n",
        "        if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
        "            print(f\"LSTM: No sequences created (train_seq: {len(X_train_seq)}, test_seq: {len(X_test_seq)})\")\n",
        "            return None\n",
        "        \n",
        "        # Build model\n",
        "        model = Sequential([\n",
        "            LSTM(32, return_sequences=True, input_shape=(seq_length, X_train.shape[1])),\n",
        "            Dropout(0.2),\n",
        "            LSTM(16, return_sequences=False),\n",
        "            Dropout(0.2),\n",
        "            Dense(8),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        \n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        \n",
        "        # Train model\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        history = model.fit(\n",
        "            X_train_seq, y_train_seq,\n",
        "            epochs=50,\n",
        "            batch_size=16,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test_seq, verbose=0).flatten()\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(y_test_seq, y_pred)\n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        return {\n",
        "            'Model_Type': 'Deep Learning',\n",
        "            'Model_Name': 'LSTM',\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'R2': metrics['R2'],\n",
        "            'Training_Time': training_time,\n",
        "            'Predictions': y_pred\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"LSTM failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def train_gru_model(X_train, y_train, X_test, y_test, seq_length=6):\n",
        "    \"\"\"\n",
        "    Train GRU model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if we have enough data for sequences\n",
        "        if len(X_train) < seq_length + 1 or len(X_test) < seq_length + 1:\n",
        "            print(f\"GRU: Insufficient data for sequences (train: {len(X_train)}, test: {len(X_test)})\")\n",
        "            return None\n",
        "        \n",
        "        # Create sequences\n",
        "        X_train_seq, y_train_seq = create_sequences(X_train, y_train, seq_length)\n",
        "        X_test_seq, y_test_seq = create_sequences(X_test, y_test, seq_length)\n",
        "        \n",
        "        if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
        "            print(f\"GRU: No sequences created (train_seq: {len(X_train_seq)}, test_seq: {len(X_test_seq)})\")\n",
        "            return None\n",
        "        \n",
        "        # Build model\n",
        "        model = Sequential([\n",
        "            GRU(32, return_sequences=True, input_shape=(seq_length, X_train.shape[1])),\n",
        "            Dropout(0.2),\n",
        "            GRU(16, return_sequences=False),\n",
        "            Dropout(0.2),\n",
        "            Dense(8),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        \n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        \n",
        "        # Train model\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        history = model.fit(\n",
        "            X_train_seq, y_train_seq,\n",
        "            epochs=50,\n",
        "            batch_size=16,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test_seq, verbose=0).flatten()\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(y_test_seq, y_pred)\n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        return {\n",
        "            'Model_Type': 'Deep Learning',\n",
        "            'Model_Name': 'GRU',\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'R2': metrics['R2'],\n",
        "            'Training_Time': training_time,\n",
        "            'Predictions': y_pred\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"GRU failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def train_ffn_model(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train Feed-Forward Neural Network\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Build model\n",
        "        model = Sequential([\n",
        "            Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "            Dense(64, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        \n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        \n",
        "        # Train model\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(y_test, y_pred)\n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        return {\n",
        "            'Model_Type': 'Deep Learning',\n",
        "            'Model_Name': 'Feed-Forward NN',\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'R2': metrics['R2'],\n",
        "            'Training_Time': training_time,\n",
        "            'Predictions': y_pred\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Feed-Forward NN failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "print(\"Deep learning models functions created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time series models functions created successfully!\n"
          ]
        }
      ],
      "source": [
        "def train_arima_model(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Train ARIMA model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from statsmodels.tsa.arima.model import ARIMA\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Simple ARIMA model with basic parameters\n",
        "        model = ARIMA(train_df['QTY'], order=(1, 1, 1))\n",
        "        fitted_model = model.fit()\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = fitted_model.forecast(steps=len(test_df))\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(test_df['QTY'], y_pred)\n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        return {\n",
        "            'Model_Type': 'Time Series',\n",
        "            'Model_Name': 'ARIMA',\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'R2': metrics['R2'],\n",
        "            'Training_Time': training_time,\n",
        "            'Predictions': y_pred\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"ARIMA failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def train_sarima_model(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Train SARIMA model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Simple SARIMA model with basic parameters\n",
        "        model = SARIMAX(train_df['QTY'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
        "        fitted_model = model.fit(disp=False)\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = fitted_model.forecast(steps=len(test_df))\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(test_df['QTY'], y_pred)\n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        return {\n",
        "            'Model_Type': 'Time Series',\n",
        "            'Model_Name': 'SARIMA',\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'R2': metrics['R2'],\n",
        "            'Training_Time': training_time,\n",
        "            'Predictions': y_pred\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"SARIMA failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def train_prophet_model(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Train Prophet model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Prepare data for Prophet\n",
        "        prophet_train = pd.DataFrame({\n",
        "            'ds': train_df['date'],\n",
        "            'y': train_df['QTY']\n",
        "        })\n",
        "        \n",
        "        prophet_test = pd.DataFrame({\n",
        "            'ds': test_df['date']\n",
        "        })\n",
        "        \n",
        "        # Initialize and fit Prophet\n",
        "        model = Prophet(\n",
        "            yearly_seasonality=True,\n",
        "            weekly_seasonality=False,\n",
        "            daily_seasonality=False\n",
        "        )\n",
        "        \n",
        "        model.fit(prophet_train)\n",
        "        \n",
        "        # Make predictions\n",
        "        forecast = model.predict(prophet_test)\n",
        "        y_pred = forecast['yhat'].values\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(test_df['QTY'], y_pred)\n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        return {\n",
        "            'Model_Type': 'Time Series',\n",
        "            'Model_Name': 'Prophet',\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'R2': metrics['R2'],\n",
        "            'Training_Time': training_time,\n",
        "            'Predictions': y_pred\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Prophet failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def train_ets_model(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Train Exponential Smoothing model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Check if we have enough data for seasonal model\n",
        "        if len(train_df) < 24:  # Need at least 2 seasonal cycles\n",
        "            # Use simple exponential smoothing\n",
        "            model = ExponentialSmoothing(\n",
        "                train_df['QTY'],\n",
        "                trend='add',\n",
        "                seasonal=None\n",
        "            )\n",
        "        else:\n",
        "            # Fit Holt-Winters model\n",
        "            model = ExponentialSmoothing(\n",
        "                train_df['QTY'],\n",
        "                trend='add',\n",
        "                seasonal='add',\n",
        "                seasonal_periods=12\n",
        "            )\n",
        "        \n",
        "        fitted_model = model.fit()\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = fitted_model.forecast(len(test_df))\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(test_df['QTY'], y_pred)\n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        return {\n",
        "            'Model_Type': 'Time Series',\n",
        "            'Model_Name': 'ETS',\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'R2': metrics['R2'],\n",
        "            'Training_Time': training_time,\n",
        "            'Predictions': y_pred\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"ETS failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def train_timegpt_model(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Train TimeGPT model using Nixtla client\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use existing Nixtla client from the original notebook\n",
        "        API_KEY = \"nixak-BPWuiu0QLaDocnyGH7oFOutH821mnpHI5jFwgujKGyPGiLCAqNkQGUQ0vp11ZSOXX9msKcsCZgVM8cRu\"\n",
        "        nixtla_client = NixtlaClient(api_key=API_KEY)\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Prepare data for TimeGPT - ensure consistent frequency\n",
        "        timegpt_train = pd.DataFrame({\n",
        "            'ds': pd.to_datetime(train_df['date']),\n",
        "            'y': train_df['QTY'],\n",
        "            'unique_id': 'sales'\n",
        "        })\n",
        "        \n",
        "        # Ensure the data is sorted and has consistent frequency\n",
        "        timegpt_train = timegpt_train.sort_values('ds').reset_index(drop=True)\n",
        "        \n",
        "        # Check for missing dates and fill them\n",
        "        timegpt_train = timegpt_train.set_index('ds').resample('MS').first().reset_index()\n",
        "        timegpt_train = timegpt_train.dropna()\n",
        "        \n",
        "        # Ensure we have enough data\n",
        "        if len(timegpt_train) < 12:\n",
        "            print(f\"TimeGPT: Insufficient data ({len(timegpt_train)} records)\")\n",
        "            return None\n",
        "        \n",
        "        # Forecast\n",
        "        forecast = nixtla_client.forecast(timegpt_train, h=len(test_df))\n",
        "        y_pred = forecast['TimeGPT'].values\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(test_df['QTY'], y_pred)\n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        return {\n",
        "            'Model_Type': 'Time Series',\n",
        "            'Model_Name': 'TimeGPT',\n",
        "            'MAE': metrics['MAE'],\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE': metrics['MAPE'],\n",
        "            'R2': metrics['R2'],\n",
        "            'Training_Time': training_time,\n",
        "            'Predictions': y_pred\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"TimeGPT failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "print(\"Time series models functions created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complete model pipeline function created successfully!\n"
          ]
        }
      ],
      "source": [
        "def run_all_models_for_location(df, location):\n",
        "    \"\"\"\n",
        "    Run all models for a specific location\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing Location: {location}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Prepare data for location\n",
        "    location_data = prepare_location_data(df, location)\n",
        "    \n",
        "    # Check if data preparation was successful\n",
        "    if location_data is None:\n",
        "        print(f\"Skipping {location} due to insufficient data\")\n",
        "        return []\n",
        "    \n",
        "    all_results = []\n",
        "    \n",
        "    # 1. Traditional ML Models\n",
        "    print(\"\\n--- Training Traditional ML Models ---\")\n",
        "    ml_results = train_ml_models(\n",
        "        location_data['X_train'],\n",
        "        location_data['y_train'],\n",
        "        location_data['X_test'],\n",
        "        location_data['y_test']\n",
        "    )\n",
        "    \n",
        "    for result in ml_results:\n",
        "        result['Location'] = location\n",
        "        all_results.append(result)\n",
        "    \n",
        "    # 2. Deep Learning Models\n",
        "    print(\"\\n--- Training Deep Learning Models ---\")\n",
        "    dl_models = [\n",
        "        ('LSTM', train_lstm_model),\n",
        "        ('GRU', train_gru_model),\n",
        "        ('Feed-Forward NN', train_ffn_model),\n",
        "        ('Transformer', train_transformer_model),\n",
        "        ('TCN', train_tcn_model)\n",
        "    ]\n",
        "    \n",
        "    for model_name, model_func in dl_models:\n",
        "        print(f\"\\nTraining {model_name}...\")\n",
        "        result = model_func(\n",
        "            location_data['X_train'],\n",
        "            location_data['y_train'],\n",
        "            location_data['X_test'],\n",
        "            location_data['y_test']\n",
        "        )\n",
        "        \n",
        "        if result is not None:\n",
        "            result['Location'] = location\n",
        "            all_results.append(result)\n",
        "            print(f\"✓ {model_name}: MAE={result['MAE']:.2f}, RMSE={result['RMSE']:.2f}, R²={result['R2']:.3f}\")\n",
        "        else:\n",
        "            print(f\"✗ {model_name}: Failed\")\n",
        "    \n",
        "    # 3. Time Series Models\n",
        "    print(\"\\n--- Training Time Series Models ---\")\n",
        "    ts_models = [\n",
        "        ('ARIMA', train_arima_model),\n",
        "        ('SARIMA', train_sarima_model),\n",
        "        ('Prophet', train_prophet_model),\n",
        "        ('ETS', train_ets_model),\n",
        "        ('TimeGPT', train_timegpt_model)\n",
        "    ]\n",
        "    \n",
        "    for model_name, model_func in ts_models:\n",
        "        print(f\"\\nTraining {model_name}...\")\n",
        "        result = model_func(\n",
        "            location_data['train_df'],\n",
        "            location_data['test_df']\n",
        "        )\n",
        "        \n",
        "        if result is not None:\n",
        "            result['Location'] = location\n",
        "            all_results.append(result)\n",
        "            print(f\"✓ {model_name}: MAE={result['MAE']:.2f}, RMSE={result['RMSE']:.2f}, R²={result['R2']:.3f}\")\n",
        "        else:\n",
        "            print(f\"✗ {model_name}: Failed\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "print(\"Complete model pipeline function created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Locations to process: ['Bangalore' 'Chennai' 'Cochin' 'Secunderabad' 'Sricity' 'Vijaywada']\n",
            "Total locations: 6\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PROCESSING LOCATION 1/6: Bangalore\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Processing Location: Bangalore\n",
            "============================================================\n",
            "\n",
            "--- Training Traditional ML Models ---\n",
            "✓ Linear Regression: MAE=14860.14, RMSE=36129.59, R²=-214.797\n",
            "✓ Ridge: MAE=2682.93, RMSE=3590.55, R²=-1.131\n",
            "✓ Lasso: MAE=5004.74, RMSE=7063.42, R²=-7.248\n",
            "✓ ElasticNet: MAE=2325.34, RMSE=3127.09, R²=-0.617\n",
            "✓ Decision Tree: MAE=2965.25, RMSE=3741.80, R²=-1.315\n",
            "✓ Random Forest: MAE=1692.50, RMSE=2074.75, R²=0.288\n",
            "✓ Gradient Boosting: MAE=1677.89, RMSE=2317.57, R²=0.112\n",
            "✓ AdaBoost: MAE=1255.89, RMSE=1685.67, R²=0.530\n",
            "✓ Extra Trees: MAE=1896.71, RMSE=2494.48, R²=-0.029\n",
            "✓ XGBoost: MAE=1609.18, RMSE=2117.70, R²=0.259\n",
            "✓ LightGBM: MAE=1958.58, RMSE=3020.98, R²=-0.509\n",
            "✓ CatBoost: MAE=1532.17, RMSE=2158.91, R²=0.229\n",
            "✓ SVR: MAE=2666.99, RMSE=3619.28, R²=-1.166\n",
            "✓ KNN: MAE=1721.38, RMSE=2145.26, R²=0.239\n",
            "\n",
            "--- Training Deep Learning Models ---\n",
            "\n",
            "Training LSTM...\n",
            "✓ LSTM: MAE=5135.55, RMSE=5263.57, R²=-19.999\n",
            "\n",
            "Training GRU...\n",
            "✓ GRU: MAE=5142.36, RMSE=5270.29, R²=-20.053\n",
            "\n",
            "Training Feed-Forward NN...\n",
            "✓ Feed-Forward NN: MAE=5313.97, RMSE=5848.83, R²=-4.655\n",
            "\n",
            "Training Transformer...\n",
            "✓ Transformer: MAE=5317.97, RMSE=5853.83, R²=-4.665\n",
            "\n",
            "Training TCN...\n",
            "✓ TCN: MAE=5314.34, RMSE=5848.21, R²=-4.654\n",
            "\n",
            "--- Training Time Series Models ---\n",
            "\n",
            "Training ARIMA...\n",
            "✓ ARIMA: MAE=1856.15, RMSE=2765.18, R²=-0.264\n",
            "\n",
            "Training SARIMA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Programs\\COMPANY\\Sashflow\\project_future_projection\\rnd\\.venv\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "INFO:cmdstanpy:Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ SARIMA: MAE=692.39, RMSE=1218.23, R²=0.755\n",
            "\n",
            "Training Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Prophet: MAE=1139.12, RMSE=1457.92, R²=0.649\n",
            "\n",
            "Training ETS...\n",
            "✓ ETS: MAE=1115.29, RMSE=1267.20, R²=0.735\n",
            "\n",
            "Training TimeGPT...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:nixtla.nixtla_client:Validating inputs...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TimeGPT failed: Could not infer the frequency of the time column. This could be due to inconsistent intervals. Please check your data for missing, duplicated or irregular timestamps\n",
            "✗ TimeGPT: Failed\n",
            "\n",
            "✓ Completed Bangalore: 23 models trained\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PROCESSING LOCATION 2/6: Chennai\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Processing Location: Chennai\n",
            "============================================================\n",
            "\n",
            "--- Training Traditional ML Models ---\n",
            "✓ Linear Regression: MAE=57683.29, RMSE=170509.53, R²=-903.133\n",
            "✓ Ridge: MAE=7831.98, RMSE=8532.15, R²=-1.264\n",
            "✓ Lasso: MAE=19759.15, RMSE=21051.40, R²=-12.782\n",
            "✓ ElasticNet: MAE=5489.41, RMSE=6243.20, R²=-0.212\n",
            "✓ Decision Tree: MAE=4580.83, RMSE=6449.98, R²=-0.294\n",
            "✓ Random Forest: MAE=4061.75, RMSE=4755.69, R²=0.297\n",
            "✓ Gradient Boosting: MAE=5964.87, RMSE=6747.18, R²=-0.416\n",
            "✓ AdaBoost: MAE=5104.51, RMSE=6264.54, R²=-0.220\n",
            "✓ Extra Trees: MAE=4853.09, RMSE=5645.71, R²=0.009\n",
            "✓ XGBoost: MAE=5794.46, RMSE=6612.90, R²=-0.360\n",
            "✓ LightGBM: MAE=4870.79, RMSE=6474.34, R²=-0.304\n",
            "✓ CatBoost: MAE=3442.15, RMSE=4382.76, R²=0.403\n",
            "✓ SVR: MAE=5957.58, RMSE=7934.04, R²=-0.958\n",
            "✓ KNN: MAE=3893.48, RMSE=4496.35, R²=0.371\n",
            "\n",
            "--- Training Deep Learning Models ---\n",
            "\n",
            "Training LSTM...\n",
            "✓ LSTM: MAE=13771.47, RMSE=14286.44, R²=-13.130\n",
            "\n",
            "Training GRU...\n",
            "✓ GRU: MAE=13778.29, RMSE=14292.86, R²=-13.143\n",
            "\n",
            "Training Feed-Forward NN...\n",
            "✓ Feed-Forward NN: MAE=15438.57, RMSE=16443.21, R²=-7.408\n",
            "\n",
            "Training Transformer...\n",
            "✓ Transformer: MAE=15439.97, RMSE=16444.08, R²=-7.409\n",
            "\n",
            "Training TCN...\n",
            "✓ TCN: MAE=15441.81, RMSE=16447.05, R²=-7.412\n",
            "\n",
            "--- Training Time Series Models ---\n",
            "\n",
            "Training ARIMA...\n",
            "✓ ARIMA: MAE=4966.11, RMSE=6645.69, R²=-0.373\n",
            "\n",
            "Training SARIMA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Programs\\COMPANY\\Sashflow\\project_future_projection\\rnd\\.venv\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "INFO:cmdstanpy:Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ SARIMA: MAE=6291.24, RMSE=7511.02, R²=-0.754\n",
            "\n",
            "Training Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:nixtla.nixtla_client:Validating inputs...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Prophet: MAE=3802.76, RMSE=5273.51, R²=0.135\n",
            "\n",
            "Training ETS...\n",
            "✓ ETS: MAE=3187.25, RMSE=5075.99, R²=0.199\n",
            "\n",
            "Training TimeGPT...\n",
            "TimeGPT failed: Could not infer the frequency of the time column. This could be due to inconsistent intervals. Please check your data for missing, duplicated or irregular timestamps\n",
            "✗ TimeGPT: Failed\n",
            "\n",
            "✓ Completed Chennai: 23 models trained\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PROCESSING LOCATION 3/6: Cochin\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Processing Location: Cochin\n",
            "============================================================\n",
            "\n",
            "--- Training Traditional ML Models ---\n",
            "✓ Linear Regression: MAE=7265.34, RMSE=9653.54, R²=-5.202\n",
            "✓ Ridge: MAE=4150.04, RMSE=4636.34, R²=-0.430\n",
            "✓ Lasso: MAE=5033.39, RMSE=6166.11, R²=-1.530\n",
            "✓ ElasticNet: MAE=3910.23, RMSE=4456.21, R²=-0.321\n",
            "✓ Decision Tree: MAE=3630.33, RMSE=4765.64, R²=-0.511\n",
            "✓ Random Forest: MAE=3306.09, RMSE=4180.62, R²=-0.163\n",
            "✓ Gradient Boosting: MAE=3673.31, RMSE=4663.37, R²=-0.447\n",
            "✓ AdaBoost: MAE=3246.90, RMSE=3960.00, R²=-0.044\n",
            "✓ Extra Trees: MAE=2911.54, RMSE=3687.01, R²=0.095\n",
            "✓ XGBoost: MAE=3648.53, RMSE=4441.98, R²=-0.313\n",
            "✓ LightGBM: MAE=3784.35, RMSE=5194.87, R²=-0.796\n",
            "✓ CatBoost: MAE=3546.67, RMSE=4440.97, R²=-0.312\n",
            "✓ SVR: MAE=4153.15, RMSE=5578.06, R²=-1.071\n",
            "✓ KNN: MAE=3393.23, RMSE=4344.67, R²=-0.256\n",
            "\n",
            "--- Training Deep Learning Models ---\n",
            "\n",
            "Training LSTM...\n",
            "✓ LSTM: MAE=6324.94, RMSE=7072.38, R²=-4.006\n",
            "\n",
            "Training GRU...\n",
            "✓ GRU: MAE=6325.85, RMSE=7072.49, R²=-4.006\n",
            "\n",
            "Training Feed-Forward NN...\n",
            "✓ Feed-Forward NN: MAE=6558.26, RMSE=7603.15, R²=-2.847\n",
            "\n",
            "Training Transformer...\n",
            "✓ Transformer: MAE=6568.32, RMSE=7617.27, R²=-2.861\n",
            "\n",
            "Training TCN...\n",
            "✓ TCN: MAE=6566.15, RMSE=7614.71, R²=-2.859\n",
            "\n",
            "--- Training Time Series Models ---\n",
            "\n",
            "Training ARIMA...\n",
            "✓ ARIMA: MAE=3822.22, RMSE=5214.62, R²=-0.810\n",
            "\n",
            "Training SARIMA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Programs\\COMPANY\\Sashflow\\project_future_projection\\rnd\\.venv\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "INFO:cmdstanpy:Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ SARIMA: MAE=2993.84, RMSE=3880.94, R²=-0.002\n",
            "\n",
            "Training Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:nixtla.nixtla_client:Validating inputs...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Prophet: MAE=3149.16, RMSE=3524.38, R²=0.173\n",
            "\n",
            "Training ETS...\n",
            "✓ ETS: MAE=2967.50, RMSE=3654.74, R²=0.111\n",
            "\n",
            "Training TimeGPT...\n",
            "TimeGPT failed: Could not infer the frequency of the time column. This could be due to inconsistent intervals. Please check your data for missing, duplicated or irregular timestamps\n",
            "✗ TimeGPT: Failed\n",
            "\n",
            "✓ Completed Cochin: 23 models trained\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PROCESSING LOCATION 4/6: Secunderabad\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Processing Location: Secunderabad\n",
            "============================================================\n",
            "\n",
            "--- Training Traditional ML Models ---\n",
            "✓ Linear Regression: MAE=48773.99, RMSE=129418.71, R²=-196.709\n",
            "✓ Ridge: MAE=6081.55, RMSE=9143.05, R²=0.013\n",
            "✓ Lasso: MAE=6085.01, RMSE=9586.49, R²=-0.085\n",
            "✓ ElasticNet: MAE=5866.44, RMSE=9024.98, R²=0.039\n",
            "✓ Decision Tree: MAE=6868.00, RMSE=9379.35, R²=-0.038\n",
            "✓ Random Forest: MAE=5269.54, RMSE=7984.14, R²=0.248\n",
            "✓ Gradient Boosting: MAE=4672.80, RMSE=7899.09, R²=0.263\n",
            "✓ AdaBoost: MAE=5514.57, RMSE=8265.84, R²=0.193\n",
            "✓ Extra Trees: MAE=5336.78, RMSE=8121.80, R²=0.221\n",
            "✓ XGBoost: MAE=5456.77, RMSE=8271.87, R²=0.192\n",
            "✓ LightGBM: MAE=6464.74, RMSE=10675.98, R²=-0.345\n",
            "✓ CatBoost: MAE=5356.62, RMSE=8254.89, R²=0.196\n",
            "✓ SVR: MAE=8055.89, RMSE=11880.80, R²=-0.666\n",
            "✓ KNN: MAE=5897.03, RMSE=9201.16, R²=0.001\n",
            "\n",
            "--- Training Deep Learning Models ---\n",
            "\n",
            "Training LSTM...\n",
            "✓ LSTM: MAE=12253.81, RMSE=12816.10, R²=-10.653\n",
            "\n",
            "Training GRU...\n",
            "✓ GRU: MAE=12257.04, RMSE=12819.07, R²=-10.659\n",
            "\n",
            "Training Feed-Forward NN...\n",
            "✓ Feed-Forward NN: MAE=14075.48, RMSE=16810.38, R²=-2.336\n",
            "\n",
            "Training Transformer...\n",
            "✓ Transformer: MAE=14089.36, RMSE=16824.87, R²=-2.341\n",
            "\n",
            "Training TCN...\n",
            "✓ TCN: MAE=14083.38, RMSE=16816.91, R²=-2.338\n",
            "\n",
            "--- Training Time Series Models ---\n",
            "\n",
            "Training ARIMA...\n",
            "✓ ARIMA: MAE=6489.00, RMSE=10449.91, R²=-0.289\n",
            "\n",
            "Training SARIMA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:cmdstanpy:Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ SARIMA: MAE=4672.99, RMSE=7630.03, R²=0.313\n",
            "\n",
            "Training Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:nixtla.nixtla_client:Validating inputs...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Prophet: MAE=5964.81, RMSE=8817.46, R²=0.082\n",
            "\n",
            "Training ETS...\n",
            "✓ ETS: MAE=5633.33, RMSE=8224.03, R²=0.202\n",
            "\n",
            "Training TimeGPT...\n",
            "TimeGPT failed: Could not infer the frequency of the time column. This could be due to inconsistent intervals. Please check your data for missing, duplicated or irregular timestamps\n",
            "✗ TimeGPT: Failed\n",
            "\n",
            "✓ Completed Secunderabad: 23 models trained\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PROCESSING LOCATION 5/6: Sricity\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Processing Location: Sricity\n",
            "============================================================\n",
            "Warning: Sricity has insufficient data (8 records). Skipping.\n",
            "Skipping Sricity due to insufficient data\n",
            "\n",
            "✓ Completed Sricity: 0 models trained\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PROCESSING LOCATION 6/6: Vijaywada\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Processing Location: Vijaywada\n",
            "============================================================\n",
            "\n",
            "--- Training Traditional ML Models ---\n",
            "✓ Linear Regression: MAE=6528.31, RMSE=8178.90, R²=-3.581\n",
            "✓ Ridge: MAE=4395.58, RMSE=5154.27, R²=-0.819\n",
            "✓ Lasso: MAE=6629.43, RMSE=8598.78, R²=-4.063\n",
            "✓ ElasticNet: MAE=4453.55, RMSE=5203.81, R²=-0.854\n",
            "✓ Decision Tree: MAE=4355.00, RMSE=5044.51, R²=-0.743\n",
            "✓ Random Forest: MAE=3651.38, RMSE=4396.99, R²=-0.324\n",
            "✓ Gradient Boosting: MAE=3810.40, RMSE=4431.69, R²=-0.345\n",
            "✓ AdaBoost: MAE=3926.08, RMSE=4622.80, R²=-0.463\n",
            "✓ Extra Trees: MAE=3841.73, RMSE=4568.78, R²=-0.429\n",
            "✓ XGBoost: MAE=3983.33, RMSE=4637.49, R²=-0.473\n",
            "✓ LightGBM: MAE=4000.61, RMSE=5143.00, R²=-0.811\n",
            "✓ CatBoost: MAE=3873.11, RMSE=4614.70, R²=-0.458\n",
            "✓ SVR: MAE=4517.42, RMSE=5691.07, R²=-1.218\n",
            "✓ KNN: MAE=3531.90, RMSE=4464.90, R²=-0.365\n",
            "\n",
            "--- Training Deep Learning Models ---\n",
            "\n",
            "Training LSTM...\n",
            "✓ LSTM: MAE=7160.42, RMSE=7760.78, R²=-5.724\n",
            "\n",
            "Training GRU...\n",
            "✓ GRU: MAE=7160.46, RMSE=7760.82, R²=-5.724\n",
            "\n",
            "Training Feed-Forward NN...\n",
            "✓ Feed-Forward NN: MAE=7537.15, RMSE=8449.38, R²=-3.889\n",
            "\n",
            "Training Transformer...\n",
            "✓ Transformer: MAE=7537.22, RMSE=8449.50, R²=-3.889\n",
            "\n",
            "Training TCN...\n",
            "✓ TCN: MAE=7529.32, RMSE=8442.28, R²=-3.881\n",
            "\n",
            "--- Training Time Series Models ---\n",
            "\n",
            "Training ARIMA...\n",
            "✓ ARIMA: MAE=4000.59, RMSE=5126.64, R²=-0.800\n",
            "\n",
            "Training SARIMA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Programs\\COMPANY\\Sashflow\\project_future_projection\\rnd\\.venv\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "INFO:prophet:n_changepoints greater than number of observations. Using 13.\n",
            "INFO:cmdstanpy:Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ SARIMA: MAE=2788.55, RMSE=3535.57, R²=0.144\n",
            "\n",
            "Training Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:nixtla.nixtla_client:Validating inputs...\n",
            "INFO:nixtla.nixtla_client:Inferred freq: MS\n",
            "INFO:nixtla.nixtla_client:Preprocessing dataframes...\n",
            "INFO:nixtla.nixtla_client:Querying model metadata...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Prophet: MAE=4237.32, RMSE=5056.11, R²=-0.751\n",
            "\n",
            "Training ETS...\n",
            "✓ ETS: MAE=3367.38, RMSE=4082.64, R²=-0.141\n",
            "\n",
            "Training TimeGPT...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:nixtla.nixtla_client:Restricting input...\n",
            "INFO:nixtla.nixtla_client:Calling Forecast Endpoint...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ TimeGPT: MAE=3327.89, RMSE=4177.62, R²=-0.195\n",
            "\n",
            "✓ Completed Vijaywada: 24 models trained\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ALL LOCATIONS PROCESSED\n",
            "Total results: 116\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Get all unique locations\n",
        "locations = df['Location'].unique()\n",
        "print(f\"Locations to process: {locations}\")\n",
        "print(f\"Total locations: {len(locations)}\")\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# Process each location\n",
        "for i, location in enumerate(locations, 1):\n",
        "    print(f\"\\n\\n{'='*80}\")\n",
        "    print(f\"PROCESSING LOCATION {i}/{len(locations)}: {location}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    try:\n",
        "        location_results = run_all_models_for_location(df, location)\n",
        "        all_results.extend(location_results)\n",
        "        \n",
        "        print(f\"\\n✓ Completed {location}: {len(location_results)} models trained\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Failed to process {location}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n\\n{'='*80}\")\n",
        "print(f\"ALL LOCATIONS PROCESSED\")\n",
        "print(f\"Total results: {len(all_results)}\")\n",
        "print(f\"{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results DataFrame shape: (116, 8)\n",
            "\n",
            "Columns: ['Model_Type', 'Model_Name', 'MAE', 'RMSE', 'MAPE', 'R2', 'Training_Time', 'Location']\n",
            "\n",
            "First few results:\n",
            "  Model_Type         Model_Name           MAE          RMSE        MAPE  \\\n",
            "0         ML  Linear Regression  14860.138543  36129.593211  268.903096   \n",
            "1         ML              Ridge   2682.927391   3590.546076   47.407227   \n",
            "2         ML              Lasso   5004.735053   7063.421677   88.344175   \n",
            "3         ML         ElasticNet   2325.341759   3127.090399   40.864988   \n",
            "4         ML      Decision Tree   2965.250000   3741.800113   52.363026   \n",
            "\n",
            "           R2  Training_Time   Location  \n",
            "0 -214.796529       0.001509  Bangalore  \n",
            "1   -1.131274       0.002012  Bangalore  \n",
            "2   -7.247999       0.006557  Bangalore  \n",
            "3   -0.616587       0.004657  Bangalore  \n",
            "4   -1.314619       0.003016  Bangalore  \n",
            "\n",
            "Results saved to: ../outputs/model_results/location_model_comparison.csv\n"
          ]
        }
      ],
      "source": [
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "# Remove predictions column for storage (too large)\n",
        "results_df_clean = results_df.drop('Predictions', axis=1, errors='ignore')\n",
        "\n",
        "print(f\"Results DataFrame shape: {results_df_clean.shape}\")\n",
        "print(f\"\\nColumns: {results_df_clean.columns.tolist()}\")\n",
        "print(f\"\\nFirst few results:\")\n",
        "print(results_df_clean.head())\n",
        "\n",
        "# Save results\n",
        "import os\n",
        "os.makedirs('../outputs/model_results', exist_ok=True)\n",
        "results_df_clean.to_csv('../outputs/model_results/location_model_comparison.csv', index=False)\n",
        "print(f\"\\nResults saved to: ../outputs/model_results/location_model_comparison.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "CORRECTED RESULTS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "--- Best Model per Location (by MAE) ---\n",
            "         Location   Model_Type         Model_Name       MAE      RMSE     R2\n",
            "20      Bangalore  Time Series             SARIMA   692.395  1218.232  0.755\n",
            "45        Chennai  Time Series                ETS  3187.251  5075.990  0.199\n",
            "54         Cochin           ML        Extra Trees  2911.539  3687.011  0.095\n",
            "75   Secunderabad           ML  Gradient Boosting  4672.800  7899.091  0.263\n",
            "112     Vijaywada  Time Series             SARIMA  2788.553  3535.565  0.144\n",
            "\n",
            "--- Average Performance by Model Type ---\n",
            "                    MAE       RMSE      R2  Training_Time\n",
            "Model_Type                                               \n",
            "Deep Learning  9446.835  10399.396  -6.824          9.021\n",
            "ML             6085.539  10477.373 -19.525          0.084\n",
            "Time Series    3641.185   4980.449  -0.042          0.364\n",
            "\n",
            "--- Top 10 Models Overall (by MAE) ---\n",
            "     Location   Model_Type         Model_Name       MAE      RMSE     R2\n",
            "20  Bangalore  Time Series             SARIMA   692.395  1218.232  0.755\n",
            "22  Bangalore  Time Series                ETS  1115.290  1267.204  0.735\n",
            "21  Bangalore  Time Series            Prophet  1139.120  1457.916  0.649\n",
            "7   Bangalore           ML           AdaBoost  1255.887  1685.675  0.530\n",
            "11  Bangalore           ML           CatBoost  1532.170  2158.909  0.229\n",
            "9   Bangalore           ML            XGBoost  1609.183  2117.703  0.259\n",
            "6   Bangalore           ML  Gradient Boosting  1677.894  2317.565  0.112\n",
            "5   Bangalore           ML      Random Forest  1692.504  2074.746  0.288\n",
            "13  Bangalore           ML                KNN  1721.383  2145.262  0.239\n",
            "19  Bangalore  Time Series              ARIMA  1856.152  2765.182 -0.264\n",
            "\n",
            "--- Model Success Rate ---\n",
            "       Model_Type         Model_Name  Count\n",
            "0   Deep Learning    Feed-Forward NN      5\n",
            "1   Deep Learning                GRU      5\n",
            "2   Deep Learning               LSTM      5\n",
            "3   Deep Learning                TCN      5\n",
            "4   Deep Learning        Transformer      5\n",
            "5              ML           AdaBoost      5\n",
            "6              ML           CatBoost      5\n",
            "7              ML      Decision Tree      5\n",
            "8              ML         ElasticNet      5\n",
            "9              ML        Extra Trees      5\n",
            "10             ML  Gradient Boosting      5\n",
            "11             ML                KNN      5\n",
            "12             ML              Lasso      5\n",
            "13             ML           LightGBM      5\n",
            "14             ML  Linear Regression      5\n",
            "15             ML      Random Forest      5\n",
            "16             ML              Ridge      5\n",
            "17             ML                SVR      5\n",
            "18             ML            XGBoost      5\n",
            "19    Time Series              ARIMA      5\n",
            "20    Time Series                ETS      5\n",
            "21    Time Series            Prophet      5\n",
            "22    Time Series             SARIMA      5\n",
            "23    Time Series            TimeGPT      1\n"
          ]
        }
      ],
      "source": [
        "# Create summary statistics for corrected results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CORRECTED RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Best model per location\n",
        "print(\"\\n--- Best Model per Location (by MAE) ---\")\n",
        "best_per_location = results_df_clean.loc[results_df_clean.groupby('Location')['MAE'].idxmin()]\n",
        "print(best_per_location[['Location', 'Model_Type', 'Model_Name', 'MAE', 'RMSE', 'R2']].round(3))\n",
        "\n",
        "# Average performance by model type\n",
        "print(\"\\n--- Average Performance by Model Type ---\")\n",
        "avg_by_type = results_df_clean.groupby('Model_Type').agg({\n",
        "    'MAE': 'mean',\n",
        "    'RMSE': 'mean',\n",
        "    'R2': 'mean',\n",
        "    'Training_Time': 'mean'\n",
        "}).round(3)\n",
        "print(avg_by_type)\n",
        "\n",
        "# Top 10 models overall\n",
        "print(\"\\n--- Top 10 Models Overall (by MAE) ---\")\n",
        "top_10 = results_df_clean.nsmallest(10, 'MAE')[['Location', 'Model_Type', 'Model_Name', 'MAE', 'RMSE', 'R2']]\n",
        "print(top_10.round(3))\n",
        "\n",
        "# Model success rate\n",
        "print(\"\\n--- Model Success Rate ---\")\n",
        "success_rate = results_df_clean.groupby(['Model_Type', 'Model_Name']).size().reset_index(name='Count')\n",
        "print(success_rate)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
