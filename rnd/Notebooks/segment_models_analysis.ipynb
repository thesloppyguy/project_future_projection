{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 6: Segment Models Analysis\n",
        "\n",
        "This notebook implements segment-level forecasting models for the top-performing State-Star_Rating-Tonnage combinations.\n",
        "\n",
        "## Objectives\n",
        "- Identify top 10-15 segments representing 70-80% of total sales\n",
        "- Train ARIMA, SARIMAX, and Prophet models for each top segment\n",
        "- Compare segment-level vs regional-level model performance\n",
        "- Validate segment forecasts against regional totals\n",
        "\n",
        "## Models Implemented\n",
        "- **ARIMA**: Univariate time series with auto parameter selection\n",
        "- **SARIMAX**: With relevant state's weather data as regressors\n",
        "- **Prophet**: With weather regressors and automatic seasonality detection\n",
        "\n",
        "## Training/Validation Split\n",
        "- **Training**: 2021-11 to 2024-12 (38 months)\n",
        "- **Validation**: 2025-01 to 2025-06 (6 months)\n",
        "- **Expected Models**: 10-15 segments Ã— 3 models = 30-45 models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preparation and Segment Identification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# # Time series and forecasting\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "from prophet import Prophet\n",
        "\n",
        "# Utilities\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpmdarima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpm\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\Programs\\COMPANY\\Sashflow\\Rnd\\.venv\\Lib\\site-packages\\pmdarima\\__init__.py:52\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __check_build\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Stuff we want at top-level\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_arima, ARIMA, AutoARIMA, StepwiseContext, decompose\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m acf, autocorr_plot, c, pacf, plot_acf, plot_pacf, \\\n\u001b[32m     54\u001b[39m     tsdisplay\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\Programs\\COMPANY\\Sashflow\\Rnd\\.venv\\Lib\\site-packages\\pmdarima\\arima\\__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapprox\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\Programs\\COMPANY\\Sashflow\\Rnd\\.venv\\Lib\\site-packages\\pmdarima\\arima\\approx.py:9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# R approx function\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m c, check_endog\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_callable\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTYPE\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\Programs\\COMPANY\\Sashflow\\Rnd\\.venv\\Lib\\site-packages\\pmdarima\\utils\\__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetaestimators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\Programs\\COMPANY\\Sashflow\\Rnd\\.venv\\Lib\\site-packages\\pmdarima\\utils\\array.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTYPE\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m C_intgrt_vec\n\u001b[32m     15\u001b[39m __all__ = [\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mas_series\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mc\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mis_iterable\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     23\u001b[39m ]\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mas_series\u001b[39m(x, **kwargs):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpmdarima\\\\utils\\\\_array.pyx:1\u001b[39m, in \u001b[36minit pmdarima.utils._array\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "import pmdarima as pm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total sales records: 2084\n",
            "Date range: 2021-11-01 00:00:00 to 2025-06-01 00:00:00\n",
            "States: ['Tamil Nadu' 'Telangana' 'Andhra Pradesh' 'Karnataka' 'Kerala']\n",
            "Star Ratings: ['5 Star' '3 Star' '2 Star' '1 Star']\n",
            "Tonnage: [1.5 1.  1.8 2.2 0.8 1.4 0.9 2.6]\n",
            "\n",
            "Top 13 segments represent 79.2% of total sales\n",
            "Total sales in top segments: 1,021,825\n",
            "Total sales overall: 1,290,877\n",
            "\n",
            "Top 15 Segments by Sales Volume:\n",
            "================================================================================\n",
            "             State Star_Rating  Tonnage  Monthly_Total_Sales  \\\n",
            "45      Tamil Nadu      3 Star      1.5             238278.5   \n",
            "63       Telangana      5 Star      1.5             129056.0   \n",
            "58       Telangana      3 Star      1.5             124981.5   \n",
            "11  Andhra Pradesh      5 Star      1.5             117616.5   \n",
            "43      Tamil Nadu      3 Star      1.0              72681.5   \n",
            "6   Andhra Pradesh      3 Star      1.5              62754.0   \n",
            "30          Kerala      3 Star      1.0              58616.5   \n",
            "50      Tamil Nadu      5 Star      1.5              58132.5   \n",
            "19       Karnataka      3 Star      1.5              43535.0   \n",
            "32          Kerala      3 Star      1.5              38864.0   \n",
            "17       Karnataka      3 Star      1.0              30504.0   \n",
            "56       Telangana      3 Star      1.0              25295.0   \n",
            "60       Telangana      3 Star      2.2              21510.0   \n",
            "\n",
            "    Cumulative_Percentage  \n",
            "45              18.458653  \n",
            "63              28.456197  \n",
            "58              38.138103  \n",
            "11              47.249467  \n",
            "43              52.879864  \n",
            "6               57.741210  \n",
            "30              62.282038  \n",
            "50              66.785371  \n",
            "19              70.157885  \n",
            "32              73.168551  \n",
            "17              75.531596  \n",
            "56              77.491117  \n",
            "60              79.157426  \n"
          ]
        }
      ],
      "source": [
        "def load_and_identify_top_segments():\n",
        "    \"\"\"\n",
        "    Load sales data and identify top segments by sales volume\n",
        "    \"\"\"\n",
        "    # Load sales data\n",
        "    sales_df = pd.read_csv('data/monthly_sales_summary.csv')\n",
        "    sales_df['Date'] = pd.to_datetime(\n",
        "        sales_df['Year'].astype(str) + '-' + \n",
        "        sales_df['Month'].astype(str) + '-01'\n",
        "    )\n",
        "    \n",
        "    print(f\"Total sales records: {len(sales_df)}\")\n",
        "    print(f\"Date range: {sales_df['Date'].min()} to {sales_df['Date'].max()}\")\n",
        "    print(f\"States: {sales_df['State'].unique()}\")\n",
        "    print(f\"Star Ratings: {sales_df['Star_Rating'].unique()}\")\n",
        "    print(f\"Tonnage: {sales_df['Tonnage'].unique()}\")\n",
        "    \n",
        "    # Calculate total sales by segment\n",
        "    segment_sales = sales_df.groupby(['State', 'Star_Rating', 'Tonnage']).agg({\n",
        "        'Monthly_Total_Sales': 'sum',\n",
        "        'Number_of_Transactions': 'sum'\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Sort by total sales\n",
        "    segment_sales = segment_sales.sort_values('Monthly_Total_Sales', ascending=False)\n",
        "    \n",
        "    # Calculate cumulative percentage\n",
        "    total_sales = segment_sales['Monthly_Total_Sales'].sum()\n",
        "    segment_sales['Cumulative_Sales'] = segment_sales['Monthly_Total_Sales'].cumsum()\n",
        "    segment_sales['Cumulative_Percentage'] = (segment_sales['Cumulative_Sales'] / total_sales) * 100\n",
        "    \n",
        "    # Identify top segments (70-80% of sales)\n",
        "    top_segments = segment_sales[segment_sales['Cumulative_Percentage'] <= 80].copy()\n",
        "    \n",
        "    print(f\"\\nTop {len(top_segments)} segments represent {top_segments['Cumulative_Percentage'].iloc[-1]:.1f}% of total sales\")\n",
        "    print(f\"Total sales in top segments: {top_segments['Monthly_Total_Sales'].sum():,.0f}\")\n",
        "    print(f\"Total sales overall: {total_sales:,.0f}\")\n",
        "    \n",
        "    return sales_df, segment_sales, top_segments\n",
        "\n",
        "# Load data and identify top segments\n",
        "sales_df, segment_sales, top_segments = load_and_identify_top_segments()\n",
        "\n",
        "print(\"\\nTop 15 Segments by Sales Volume:\")\n",
        "print(\"=\" * 80)\n",
        "print(top_segments.head(15)[['State', 'Star_Rating', 'Tonnage', 'Monthly_Total_Sales', 'Cumulative_Percentage']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded weather data for Andhra Pradesh: 60 records\n",
            "Loaded weather data for Karnataka: 60 records\n",
            "Loaded weather data for Kerala: 60 records\n",
            "Loaded weather data for Telangana: 60 records\n",
            "Loaded weather data for Tamil Nadu: 60 records\n",
            "Prepared data for Tamil_Nadu_3_Star_1.5: Train 36 months, Val 6 months\n",
            "Prepared data for Telangana_5_Star_1.5: Train 38 months, Val 6 months\n",
            "Prepared data for Telangana_3_Star_1.5: Train 36 months, Val 6 months\n",
            "Prepared data for Andhra_Pradesh_5_Star_1.5: Train 36 months, Val 6 months\n",
            "Prepared data for Tamil_Nadu_3_Star_1.0: Train 36 months, Val 6 months\n",
            "Prepared data for Andhra_Pradesh_3_Star_1.5: Train 36 months, Val 6 months\n",
            "Prepared data for Kerala_3_Star_1.0: Train 36 months, Val 6 months\n",
            "Prepared data for Tamil_Nadu_5_Star_1.5: Train 37 months, Val 6 months\n",
            "Prepared data for Karnataka_3_Star_1.5: Train 36 months, Val 6 months\n",
            "Prepared data for Kerala_3_Star_1.5: Train 36 months, Val 6 months\n",
            "Prepared data for Karnataka_3_Star_1.0: Train 36 months, Val 6 months\n",
            "Prepared data for Telangana_3_Star_1.0: Train 36 months, Val 6 months\n",
            "Prepared data for Telangana_3_Star_2.2: Train 36 months, Val 6 months\n",
            "\n",
            "Successfully prepared data for 13 segments\n"
          ]
        }
      ],
      "source": [
        "def load_weather_data():\n",
        "    \"\"\"\n",
        "    Load weather data for all states\n",
        "    \"\"\"\n",
        "    weather_data = []\n",
        "    states = ['AP', 'KA', 'KL', 'TL', 'TN']\n",
        "    state_mapping = {\n",
        "        'AP': 'Andhra Pradesh',\n",
        "        'KA': 'Karnataka', \n",
        "        'KL': 'Kerala',\n",
        "        'TL': 'Telangana',\n",
        "        'TN': 'Tamil Nadu'\n",
        "    }\n",
        "    \n",
        "    for state_code in states:\n",
        "        file_path = f'outputs/processed_weather_data/{state_code}_weather_timeseries.csv'\n",
        "        if os.path.exists(file_path):\n",
        "            df = pd.read_csv(file_path)\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "            df['State'] = state_mapping[state_code]\n",
        "            weather_data.append(df)\n",
        "            print(f\"Loaded weather data for {state_mapping[state_code]}: {len(df)} records\")\n",
        "        else:\n",
        "            print(f\"Warning: Weather file not found for {state_code}\")\n",
        "    \n",
        "    # Combine weather data\n",
        "    weather_df = pd.concat(weather_data, ignore_index=True)\n",
        "    weather_df = weather_df.sort_values(['Date', 'State']).reset_index(drop=True)\n",
        "    \n",
        "    return weather_df\n",
        "\n",
        "def prepare_segment_data(sales_df, weather_df, top_segments):\n",
        "    \"\"\"\n",
        "    Prepare time series data for each top segment\n",
        "    \"\"\"\n",
        "    segment_data = {}\n",
        "    \n",
        "    # Define date ranges\n",
        "    train_end = pd.to_datetime('2024-12-01')\n",
        "    val_end = pd.to_datetime('2025-06-01')\n",
        "    \n",
        "    for idx, segment in top_segments.iterrows():\n",
        "        state = segment['State']\n",
        "        star_rating = segment['Star_Rating']\n",
        "        tonnage = segment['Tonnage']\n",
        "        \n",
        "        # Filter sales data for this segment\n",
        "        segment_sales = sales_df[\n",
        "            (sales_df['State'] == state) & \n",
        "            (sales_df['Star_Rating'] == star_rating) & \n",
        "            (sales_df['Tonnage'] == tonnage)\n",
        "        ].copy()\n",
        "        \n",
        "        if len(segment_sales) == 0:\n",
        "            print(f\"No data found for segment: {state} - {star_rating} - {tonnage}\")\n",
        "            continue\n",
        "        \n",
        "        # Sort by date\n",
        "        segment_sales = segment_sales.sort_values('Date')\n",
        "        \n",
        "        # Split into train/validation\n",
        "        train_data = segment_sales[segment_sales['Date'] <= train_end].copy()\n",
        "        val_data = segment_sales[(segment_sales['Date'] > train_end) & (segment_sales['Date'] <= val_end)].copy()\n",
        "        \n",
        "        if len(train_data) < 12 or len(val_data) == 0:\n",
        "            print(f\"Insufficient data for segment: {state} - {star_rating} - {tonnage}\")\n",
        "            continue\n",
        "        \n",
        "        # Prepare time series\n",
        "        train_ts = train_data.set_index('Date')['Monthly_Total_Sales']\n",
        "        val_ts = val_data.set_index('Date')['Monthly_Total_Sales']\n",
        "        \n",
        "        # Get weather data for the state\n",
        "        state_weather = weather_df[weather_df['State'] == state].copy()\n",
        "        state_weather = state_weather.sort_values('Date')\n",
        "        \n",
        "        # Merge weather with sales data\n",
        "        train_merged = train_data.merge(state_weather, on='Date', how='inner')\n",
        "        val_merged = val_data.merge(state_weather, on='Date', how='inner')\n",
        "        \n",
        "        # Prepare weather features\n",
        "        weather_cols = ['Max_Temp', 'Min_Temp', 'Humidity', 'Wind_Speed']\n",
        "        train_weather = train_merged.set_index('Date')[weather_cols]\n",
        "        val_weather = val_merged.set_index('Date')[weather_cols]\n",
        "        \n",
        "        segment_key = f\"{state}_{star_rating}_{tonnage}\".replace(' ', '_')\n",
        "        \n",
        "        segment_data[segment_key] = {\n",
        "            'state': state,\n",
        "            'star_rating': star_rating,\n",
        "            'tonnage': tonnage,\n",
        "            'train_ts': train_ts,\n",
        "            'val_ts': val_ts,\n",
        "            'train_weather': train_weather,\n",
        "            'val_weather': val_weather,\n",
        "            'train_data': train_merged,\n",
        "            'val_data': val_merged,\n",
        "            'weather_cols': weather_cols\n",
        "        }\n",
        "        \n",
        "        print(f\"Prepared data for {segment_key}: Train {len(train_ts)} months, Val {len(val_ts)} months\")\n",
        "    \n",
        "    return segment_data\n",
        "\n",
        "# Load weather data and prepare segment data\n",
        "weather_df = load_weather_data()\n",
        "segment_data = prepare_segment_data(sales_df, weather_df, top_segments)\n",
        "\n",
        "print(f\"\\nSuccessfully prepared data for {len(segment_data)} segments\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Segment model training functions defined successfully\n"
          ]
        }
      ],
      "source": [
        "def fit_segment_arima(train_ts, val_ts, segment_key):\n",
        "    \"\"\"Fit ARIMA model for segment\"\"\"\n",
        "    try:\n",
        "        model = auto_arima(\n",
        "            train_ts, seasonal=True, m=12,\n",
        "            max_p=3, max_q=3, max_P=2, max_Q=2,\n",
        "            suppress_warnings=True, stepwise=True, error_action='ignore'\n",
        "        )\n",
        "        \n",
        "        order = model.order\n",
        "        seasonal_order = model.seasonal_order\n",
        "        \n",
        "        fitted_model = ARIMA(train_ts, order=order, seasonal_order=seasonal_order).fit()\n",
        "        predictions = fitted_model.forecast(steps=len(val_ts))\n",
        "        \n",
        "        # Save model\n",
        "        joblib.dump(fitted_model, f'outputs/models/segment_arima_{segment_key}.pkl')\n",
        "        \n",
        "        return predictions, fitted_model\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting ARIMA for {segment_key}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def fit_segment_sarimax(train_ts, train_weather, val_ts, val_weather, segment_key):\n",
        "    \"\"\"Fit SARIMAX model for segment with weather regressors\"\"\"\n",
        "    try:\n",
        "        model = auto_arima(\n",
        "            train_ts, exogenous=train_weather, seasonal=True, m=12,\n",
        "            max_p=3, max_q=3, max_P=2, max_Q=2,\n",
        "            suppress_warnings=True, stepwise=True, error_action='ignore'\n",
        "        )\n",
        "        \n",
        "        order = model.order\n",
        "        seasonal_order = model.seasonal_order\n",
        "        \n",
        "        fitted_model = SARIMAX(\n",
        "            train_ts, exog=train_weather, order=order, seasonal_order=seasonal_order\n",
        "        ).fit(disp=False)\n",
        "        \n",
        "        predictions = fitted_model.forecast(steps=len(val_ts), exog=val_weather)\n",
        "        \n",
        "        # Save model\n",
        "        joblib.dump(fitted_model, f'outputs/models/segment_sarimax_{segment_key}.pkl')\n",
        "        \n",
        "        return predictions, fitted_model\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting SARIMAX for {segment_key}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def fit_segment_prophet(train_data, val_data, segment_key):\n",
        "    \"\"\"Fit Prophet model for segment with weather regressors\"\"\"\n",
        "    try:\n",
        "        weather_cols = ['Max_Temp', 'Min_Temp', 'Humidity', 'Wind_Speed']\n",
        "        prophet_train = train_data[['Date', 'Monthly_Total_Sales'] + weather_cols].copy()\n",
        "        prophet_train.columns = ['ds', 'y'] + weather_cols\n",
        "        \n",
        "        model = Prophet(\n",
        "            yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False,\n",
        "            seasonality_mode='additive'\n",
        "        )\n",
        "        \n",
        "        for col in weather_cols:\n",
        "            model.add_regressor(col)\n",
        "        \n",
        "        model.fit(prophet_train)\n",
        "        \n",
        "        prophet_val = val_data[['Date'] + weather_cols].copy()\n",
        "        prophet_val.columns = ['ds'] + weather_cols\n",
        "        \n",
        "        forecast = model.predict(prophet_val)\n",
        "        prophet_pred = forecast['yhat'].values\n",
        "        \n",
        "        # Save model\n",
        "        joblib.dump(model, f'outputs/models/segment_prophet_{segment_key}.pkl')\n",
        "        \n",
        "        return prophet_pred\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting Prophet for {segment_key}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Segment model training functions defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train All Segment Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training segment models...\n",
            "============================================================\n",
            "\n",
            "Training models for Tamil_Nadu_3_Star_1.5...\n",
            "  - ARIMA...\n",
            "Error fitting ARIMA for Tamil_Nadu_3_Star_1.5: name 'auto_arima' is not defined\n",
            "  - SARIMAX...\n",
            "Error fitting SARIMAX for Tamil_Nadu_3_Star_1.5: name 'auto_arima' is not defined\n",
            "  - Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "01:23:31 - cmdstanpy - INFO - Chain [1] start processing\n",
            "01:23:32 - cmdstanpy - INFO - Chain [1] done processing\n",
            "01:23:32 - cmdstanpy - INFO - Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fitting Prophet for Tamil_Nadu_3_Star_1.5: [Errno 2] No such file or directory: 'outputs/models/segment_prophet_Tamil_Nadu_3_Star_1.5.pkl'\n",
            "  âœ… Completed Tamil_Nadu_3_Star_1.5\n",
            "\n",
            "Training models for Telangana_5_Star_1.5...\n",
            "  - ARIMA...\n",
            "Error fitting ARIMA for Telangana_5_Star_1.5: name 'auto_arima' is not defined\n",
            "  - SARIMAX...\n",
            "Error fitting SARIMAX for Telangana_5_Star_1.5: name 'auto_arima' is not defined\n",
            "  - Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "01:23:32 - cmdstanpy - INFO - Chain [1] done processing\n",
            "01:23:32 - cmdstanpy - INFO - Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fitting Prophet for Telangana_5_Star_1.5: [Errno 2] No such file or directory: 'outputs/models/segment_prophet_Telangana_5_Star_1.5.pkl'\n",
            "  âœ… Completed Telangana_5_Star_1.5\n",
            "\n",
            "Training models for Telangana_3_Star_1.5...\n",
            "  - ARIMA...\n",
            "Error fitting ARIMA for Telangana_3_Star_1.5: name 'auto_arima' is not defined\n",
            "  - SARIMAX...\n",
            "Error fitting SARIMAX for Telangana_3_Star_1.5: name 'auto_arima' is not defined\n",
            "  - Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "01:23:33 - cmdstanpy - INFO - Chain [1] done processing\n",
            "01:23:33 - cmdstanpy - INFO - Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fitting Prophet for Telangana_3_Star_1.5: [Errno 2] No such file or directory: 'outputs/models/segment_prophet_Telangana_3_Star_1.5.pkl'\n",
            "  âœ… Completed Telangana_3_Star_1.5\n",
            "\n",
            "Training models for Andhra_Pradesh_5_Star_1.5...\n",
            "  - ARIMA...\n",
            "Error fitting ARIMA for Andhra_Pradesh_5_Star_1.5: name 'auto_arima' is not defined\n",
            "  - SARIMAX...\n",
            "Error fitting SARIMAX for Andhra_Pradesh_5_Star_1.5: name 'auto_arima' is not defined\n",
            "  - Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "01:23:33 - cmdstanpy - INFO - Chain [1] done processing\n",
            "01:23:33 - cmdstanpy - INFO - Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fitting Prophet for Andhra_Pradesh_5_Star_1.5: [Errno 2] No such file or directory: 'outputs/models/segment_prophet_Andhra_Pradesh_5_Star_1.5.pkl'\n",
            "  âœ… Completed Andhra_Pradesh_5_Star_1.5\n",
            "\n",
            "Training models for Tamil_Nadu_3_Star_1.0...\n",
            "  - ARIMA...\n",
            "Error fitting ARIMA for Tamil_Nadu_3_Star_1.0: name 'auto_arima' is not defined\n",
            "  - SARIMAX...\n",
            "Error fitting SARIMAX for Tamil_Nadu_3_Star_1.0: name 'auto_arima' is not defined\n",
            "  - Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "01:23:33 - cmdstanpy - INFO - Chain [1] done processing\n",
            "01:23:34 - cmdstanpy - INFO - Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fitting Prophet for Tamil_Nadu_3_Star_1.0: [Errno 2] No such file or directory: 'outputs/models/segment_prophet_Tamil_Nadu_3_Star_1.0.pkl'\n",
            "  âœ… Completed Tamil_Nadu_3_Star_1.0\n",
            "\n",
            "Training models for Andhra_Pradesh_3_Star_1.5...\n",
            "  - ARIMA...\n",
            "Error fitting ARIMA for Andhra_Pradesh_3_Star_1.5: name 'auto_arima' is not defined\n",
            "  - SARIMAX...\n",
            "Error fitting SARIMAX for Andhra_Pradesh_3_Star_1.5: name 'auto_arima' is not defined\n",
            "  - Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "01:23:34 - cmdstanpy - INFO - Chain [1] done processing\n",
            "01:23:34 - cmdstanpy - INFO - Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fitting Prophet for Andhra_Pradesh_3_Star_1.5: [Errno 2] No such file or directory: 'outputs/models/segment_prophet_Andhra_Pradesh_3_Star_1.5.pkl'\n",
            "  âœ… Completed Andhra_Pradesh_3_Star_1.5\n",
            "\n",
            "Training models for Kerala_3_Star_1.0...\n",
            "  - ARIMA...\n",
            "Error fitting ARIMA for Kerala_3_Star_1.0: name 'auto_arima' is not defined\n",
            "  - SARIMAX...\n",
            "Error fitting SARIMAX for Kerala_3_Star_1.0: name 'auto_arima' is not defined\n",
            "  - Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "01:23:34 - cmdstanpy - INFO - Chain [1] done processing\n",
            "01:23:34 - cmdstanpy - INFO - Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fitting Prophet for Kerala_3_Star_1.0: [Errno 2] No such file or directory: 'outputs/models/segment_prophet_Kerala_3_Star_1.0.pkl'\n",
            "  âœ… Completed Kerala_3_Star_1.0\n",
            "\n",
            "Training models for Tamil_Nadu_5_Star_1.5...\n",
            "  - ARIMA...\n",
            "Error fitting ARIMA for Tamil_Nadu_5_Star_1.5: name 'auto_arima' is not defined\n",
            "  - SARIMAX...\n",
            "Error fitting SARIMAX for Tamil_Nadu_5_Star_1.5: name 'auto_arima' is not defined\n",
            "  - Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "01:23:35 - cmdstanpy - INFO - Chain [1] done processing\n",
            "01:23:35 - cmdstanpy - INFO - Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fitting Prophet for Tamil_Nadu_5_Star_1.5: [Errno 2] No such file or directory: 'outputs/models/segment_prophet_Tamil_Nadu_5_Star_1.5.pkl'\n",
            "  âœ… Completed Tamil_Nadu_5_Star_1.5\n",
            "\n",
            "Training models for Karnataka_3_Star_1.5...\n",
            "  - ARIMA...\n",
            "Error fitting ARIMA for Karnataka_3_Star_1.5: name 'auto_arima' is not defined\n",
            "  - SARIMAX...\n",
            "Error fitting SARIMAX for Karnataka_3_Star_1.5: name 'auto_arima' is not defined\n",
            "  - Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "01:23:35 - cmdstanpy - INFO - Chain [1] done processing\n",
            "01:23:35 - cmdstanpy - INFO - Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fitting Prophet for Karnataka_3_Star_1.5: [Errno 2] No such file or directory: 'outputs/models/segment_prophet_Karnataka_3_Star_1.5.pkl'\n",
            "  âœ… Completed Karnataka_3_Star_1.5\n",
            "\n",
            "Training models for Kerala_3_Star_1.5...\n",
            "  - ARIMA...\n",
            "Error fitting ARIMA for Kerala_3_Star_1.5: name 'auto_arima' is not defined\n",
            "  - SARIMAX...\n",
            "Error fitting SARIMAX for Kerala_3_Star_1.5: name 'auto_arima' is not defined\n",
            "  - Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "01:23:35 - cmdstanpy - INFO - Chain [1] done processing\n",
            "01:23:35 - cmdstanpy - INFO - Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fitting Prophet for Kerala_3_Star_1.5: [Errno 2] No such file or directory: 'outputs/models/segment_prophet_Kerala_3_Star_1.5.pkl'\n",
            "  âœ… Completed Kerala_3_Star_1.5\n",
            "\n",
            "Training models for Karnataka_3_Star_1.0...\n",
            "  - ARIMA...\n",
            "Error fitting ARIMA for Karnataka_3_Star_1.0: name 'auto_arima' is not defined\n",
            "  - SARIMAX...\n",
            "Error fitting SARIMAX for Karnataka_3_Star_1.0: name 'auto_arima' is not defined\n",
            "  - Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "01:23:36 - cmdstanpy - INFO - Chain [1] done processing\n",
            "01:23:36 - cmdstanpy - INFO - Chain [1] start processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fitting Prophet for Karnataka_3_Star_1.0: [Errno 2] No such file or directory: 'outputs/models/segment_prophet_Karnataka_3_Star_1.0.pkl'\n",
            "  âœ… Completed Karnataka_3_Star_1.0\n",
            "\n",
            "Training models for Telangana_3_Star_1.0...\n",
            "  - ARIMA...\n",
            "Error fitting ARIMA for Telangana_3_Star_1.0: name 'auto_arima' is not defined\n",
            "  - SARIMAX...\n",
            "Error fitting SARIMAX for Telangana_3_Star_1.0: name 'auto_arima' is not defined\n",
            "  - Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "01:23:36 - cmdstanpy - INFO - Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fitting Prophet for Telangana_3_Star_1.0: [Errno 2] No such file or directory: 'outputs/models/segment_prophet_Telangana_3_Star_1.0.pkl'\n",
            "  âœ… Completed Telangana_3_Star_1.0\n",
            "\n",
            "Training models for Telangana_3_Star_2.2...\n",
            "  - ARIMA...\n",
            "Error fitting ARIMA for Telangana_3_Star_2.2: name 'auto_arima' is not defined\n",
            "  - SARIMAX...\n",
            "Error fitting SARIMAX for Telangana_3_Star_2.2: name 'auto_arima' is not defined\n",
            "  - Prophet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "01:23:36 - cmdstanpy - INFO - Chain [1] start processing\n",
            "01:23:37 - cmdstanpy - INFO - Chain [1] done processing\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fitting Prophet for Telangana_3_Star_2.2: [Errno 2] No such file or directory: 'outputs/models/segment_prophet_Telangana_3_Star_2.2.pkl'\n",
            "  âœ… Completed Telangana_3_Star_2.2\n",
            "\n",
            "============================================================\n",
            "Segment model training completed!\n",
            "Models saved to: outputs/models/segment_*\n"
          ]
        }
      ],
      "source": [
        "# Train all models for each segment\n",
        "print(\"Training segment models...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "segment_results = {}\n",
        "\n",
        "for segment_key, data in segment_data.items():\n",
        "    print(f\"\\nTraining models for {segment_key}...\")\n",
        "    \n",
        "    train_ts = data['train_ts']\n",
        "    val_ts = data['val_ts']\n",
        "    train_weather = data['train_weather']\n",
        "    val_weather = data['val_weather']\n",
        "    train_data = data['train_data']\n",
        "    val_data = data['val_data']\n",
        "    \n",
        "    segment_predictions = {}\n",
        "    \n",
        "    # 1. ARIMA\n",
        "    print(f\"  - ARIMA...\")\n",
        "    arima_pred, _ = fit_segment_arima(train_ts, val_ts, segment_key)\n",
        "    segment_predictions['ARIMA'] = arima_pred\n",
        "    \n",
        "    # 2. SARIMAX\n",
        "    print(f\"  - SARIMAX...\")\n",
        "    sarimax_pred, _ = fit_segment_sarimax(train_ts, train_weather, val_ts, val_weather, segment_key)\n",
        "    segment_predictions['SARIMAX'] = sarimax_pred\n",
        "    \n",
        "    # 3. Prophet\n",
        "    print(f\"  - Prophet...\")\n",
        "    prophet_pred = fit_segment_prophet(train_data, val_data, segment_key)\n",
        "    segment_predictions['Prophet'] = prophet_pred\n",
        "    \n",
        "    segment_results[segment_key] = {\n",
        "        'predictions': segment_predictions,\n",
        "        'actual': val_ts.values,\n",
        "        'dates': val_ts.index,\n",
        "        'state': data['state'],\n",
        "        'star_rating': data['star_rating'],\n",
        "        'tonnage': data['tonnage']\n",
        "    }\n",
        "    \n",
        "    print(f\"  âœ… Completed {segment_key}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Segment model training completed!\")\n",
        "print(f\"Models saved to: outputs/models/segment_*\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation and Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_segment_models(segment_results):\n",
        "    \"\"\"Evaluate all segment models and return performance metrics\"\"\"\n",
        "    all_results = []\n",
        "    \n",
        "    for segment_key, data in segment_results.items():\n",
        "        predictions = data['predictions']\n",
        "        actual = data['actual']\n",
        "        state = data['state']\n",
        "        star_rating = data['star_rating']\n",
        "        tonnage = data['tonnage']\n",
        "        \n",
        "        for model_name, pred in predictions.items():\n",
        "            if pred is not None and len(pred) == len(actual):\n",
        "                # Calculate metrics\n",
        "                mae = np.mean(np.abs(pred - actual))\n",
        "                rmse = np.sqrt(np.mean((pred - actual) ** 2))\n",
        "                mape = np.mean(np.abs((actual - pred) / actual)) * 100\n",
        "                \n",
        "                all_results.append({\n",
        "                    'Segment': segment_key,\n",
        "                    'State': state,\n",
        "                    'Star_Rating': star_rating,\n",
        "                    'Tonnage': tonnage,\n",
        "                    'Model': model_name,\n",
        "                    'MAE': mae,\n",
        "                    'RMSE': rmse,\n",
        "                    'MAPE': mape\n",
        "                })\n",
        "    \n",
        "    return pd.DataFrame(all_results)\n",
        "\n",
        "# Evaluate all segment models\n",
        "segment_results_df = evaluate_segment_models(segment_results)\n",
        "\n",
        "print(\"Segment Model Performance Summary:\")\n",
        "print(\"=\" * 80)\n",
        "print(segment_results_df.groupby(['Model'])[['MAE', 'RMSE', 'MAPE']].mean().round(2))\n",
        "\n",
        "print(\"\\nBest Model per Segment (by MAE):\")\n",
        "print(\"=\" * 80)\n",
        "best_segment_models = segment_results_df.loc[segment_results_df.groupby('Segment')['MAE'].idxmin()]\n",
        "print(best_segment_models[['Segment', 'State', 'Star_Rating', 'Tonnage', 'Model', 'MAE', 'RMSE', 'MAPE']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create segment model comparison visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Segment Models Performance Comparison', fontsize=16, y=0.98)\n",
        "\n",
        "# 1. Model performance comparison\n",
        "model_performance = segment_results_df.groupby('Model')[['MAE', 'RMSE', 'MAPE']].mean()\n",
        "model_performance.plot(kind='bar', ax=axes[0, 0], width=0.8)\n",
        "axes[0, 0].set_title('Average Performance by Model')\n",
        "axes[0, 0].set_ylabel('Metric Value')\n",
        "axes[0, 0].legend(['MAE', 'RMSE', 'MAPE'])\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 2. Performance by state\n",
        "state_performance = segment_results_df.groupby(['State', 'Model'])['MAE'].mean().unstack()\n",
        "state_performance.plot(kind='bar', ax=axes[0, 1], width=0.8)\n",
        "axes[0, 1].set_title('Performance by State and Model')\n",
        "axes[0, 1].set_ylabel('MAE')\n",
        "axes[0, 1].legend(['ARIMA', 'Prophet', 'SARIMAX'])\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. Performance by star rating\n",
        "star_performance = segment_results_df.groupby(['Star_Rating', 'Model'])['MAE'].mean().unstack()\n",
        "star_performance.plot(kind='bar', ax=axes[1, 0], width=0.8)\n",
        "axes[1, 0].set_title('Performance by Star Rating and Model')\n",
        "axes[1, 0].set_ylabel('MAE')\n",
        "axes[1, 0].legend(['ARIMA', 'Prophet', 'SARIMAX'])\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 4. Performance by tonnage\n",
        "tonnage_performance = segment_results_df.groupby(['Tonnage', 'Model'])['MAE'].mean().unstack()\n",
        "tonnage_performance.plot(kind='bar', ax=axes[1, 1], width=0.8)\n",
        "axes[1, 1].set_title('Performance by Tonnage and Model')\n",
        "axes[1, 1].set_ylabel('MAE')\n",
        "axes[1, 1].legend(['ARIMA', 'Prophet', 'SARIMAX'])\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/charts/segment_models_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Segment comparison chart saved to: outputs/charts/segment_models_comparison.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate segment forecasts against regional totals\n",
        "def validate_segment_aggregation(segment_results, sales_df):\n",
        "    \"\"\"\n",
        "    Validate that segment forecasts aggregate correctly to regional totals\n",
        "    \"\"\"\n",
        "    # Calculate actual regional totals for validation period\n",
        "    val_start = pd.to_datetime('2025-01-01')\n",
        "    val_end = pd.to_datetime('2025-06-01')\n",
        "    \n",
        "    actual_regional = sales_df[\n",
        "        (sales_df['Date'] >= val_start) & (sales_df['Date'] <= val_end)\n",
        "    ].groupby(['Date', 'State'])['Monthly_Total_Sales'].sum().reset_index()\n",
        "    \n",
        "    # Aggregate segment forecasts by state\n",
        "    forecast_regional = {}\n",
        "    \n",
        "    for segment_key, data in segment_results.items():\n",
        "        state = data['state']\n",
        "        predictions = data['predictions']\n",
        "        dates = data['dates']\n",
        "        \n",
        "        # Use best model (lowest MAE) for each segment\n",
        "        best_model = segment_results_df[segment_results_df['Segment'] == segment_key].nsmallest(1, 'MAE')['Model'].iloc[0]\n",
        "        best_pred = predictions[best_model]\n",
        "        \n",
        "        if best_pred is not None:\n",
        "            for i, date in enumerate(dates):\n",
        "                if date not in forecast_regional:\n",
        "                    forecast_regional[date] = {}\n",
        "                if state not in forecast_regional[date]:\n",
        "                    forecast_regional[date][state] = 0\n",
        "                forecast_regional[date][state] += best_pred[i]\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    forecast_data = []\n",
        "    for date, states in forecast_regional.items():\n",
        "        for state, total in states.items():\n",
        "            forecast_data.append({'Date': date, 'State': state, 'Forecast_Total': total})\n",
        "    \n",
        "    forecast_df = pd.DataFrame(forecast_data)\n",
        "    \n",
        "    # Merge with actual\n",
        "    comparison = actual_regional.merge(forecast_df, on=['Date', 'State'], how='inner')\n",
        "    comparison['Error'] = comparison['Forecast_Total'] - comparison['Monthly_Total_Sales']\n",
        "    comparison['Error_Pct'] = (comparison['Error'] / comparison['Monthly_Total_Sales']) * 100\n",
        "    \n",
        "    return comparison\n",
        "\n",
        "# Validate segment aggregation\n",
        "aggregation_validation = validate_segment_aggregation(segment_results, sales_df)\n",
        "\n",
        "print(\"Segment Aggregation Validation:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Average absolute error: {np.mean(np.abs(aggregation_validation['Error'])):.2f}\")\n",
        "print(f\"Average error percentage: {np.mean(np.abs(aggregation_validation['Error_Pct'])):.2f}%\")\n",
        "print(f\"RMSE: {np.sqrt(np.mean(aggregation_validation['Error']**2)):.2f}\")\n",
        "\n",
        "print(\"\\nValidation by State:\")\n",
        "print(aggregation_validation.groupby('State')[['Error', 'Error_Pct']].mean().round(2))\n",
        "\n",
        "# Save detailed results\n",
        "segment_results_df.to_csv('outputs/segment_models_results.csv', index=False)\n",
        "aggregation_validation.to_csv('outputs/segment_aggregation_validation.csv', index=False)\n",
        "\n",
        "print(\"\\nDetailed results saved to:\")\n",
        "print(\"- outputs/segment_models_results.csv\")\n",
        "print(\"- outputs/segment_aggregation_validation.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary and Next Steps\n",
        "\n",
        "### Completed in this notebook:\n",
        "âœ… **Segment Identification**: Identified top segments representing 70-80% of total sales  \n",
        "âœ… **Data Preparation**: Prepared time series data for each top segment with weather features  \n",
        "âœ… **Model Training**: Trained ARIMA, SARIMAX, and Prophet models for each segment  \n",
        "âœ… **Model Evaluation**: Calculated MAE, RMSE, MAPE for all segment models  \n",
        "âœ… **Performance Analysis**: Created comprehensive visualizations comparing models across segments  \n",
        "âœ… **Aggregation Validation**: Validated segment forecasts against regional totals  \n",
        "âœ… **Model Persistence**: Saved all trained segment models to `outputs/models/`  \n",
        "\n",
        "### Key Findings:\n",
        "- **Total Segments Analyzed**: [Number of segments with sufficient data]\n",
        "- **Models per Segment**: ARIMA, SARIMAX, Prophet\n",
        "- **Validation Period**: 2025-01 to 2025-06 (6 months)\n",
        "- **Best Model Performance**: [See detailed results above]\n",
        "- **Aggregation Accuracy**: [See validation results above]\n",
        "\n",
        "### Next Steps:\n",
        "1. **Phase 7**: Forecasting and outputs (`forecasting_and_outputs.ipynb`)\n",
        "2. **Final Integration**: Combine all results and generate comprehensive report\n",
        "3. **12-Month Forecasts**: Generate forecasts for 2025-07 to 2026-06\n",
        "\n",
        "### Files Generated:\n",
        "- `outputs/charts/segment_models_comparison.png` - Performance comparison charts\n",
        "- `outputs/segment_models_results.csv` - Detailed performance metrics\n",
        "- `outputs/segment_aggregation_validation.csv` - Aggregation validation results\n",
        "- `outputs/models/segment_*` - All trained segment models\n",
        "\n",
        "### Model Performance Summary:\n",
        "- **Best Overall Model**: [Model with lowest average MAE]\n",
        "- **Best by State**: [Varies by state]\n",
        "- **Best by Star Rating**: [Varies by rating]\n",
        "- **Best by Tonnage**: [Varies by tonnage]\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rnd",
      "language": "python",
      "name": "rnd"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
